<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Quarkus Superheroes: Managed services save the day</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/23/quarkus-superheroes-managed-services-save-day" /><author><name>Eric Deandrea</name></author><id>fc4ac9e3-f97b-4a22-b32d-bb1728224dcc</id><updated>2022-03-23T07:00:00Z</updated><published>2022-03-23T07:00:00Z</published><summary type="html">&lt;p&gt;Are you a developer building &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;? Do you struggle with developing and testing individual microservices that are part of a more extensive system? Would you rather focus on your applications and let something else manage the services they require?&lt;/p&gt; &lt;p&gt;This article introduces the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes sample application&lt;/a&gt;, shows how to deploy it to the free &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, and then illustrates how &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; developers can modify the application to replace the backing services with fully managed services provided by &lt;a href="https://cloud.redhat.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt;. The article might seem long, but running the steps should take only 15 to 20 minutes. There are many screenshots to help guide you on your way.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Here's what you will need if you want to follow along with the steps in this article:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A Red Hat account, which you need in order to create the managed &lt;a href="https://console.redhat.com/application-services/overview"&gt;Application Services&lt;/a&gt; on the &lt;a href="https://console.redhat.com"&gt;Red Hat Hybrid Cloud Console&lt;/a&gt; and access the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. No credit card is required.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; &lt;a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/getting-started-cli.html"&gt;Red Hat OpenShift command-line interface (CLI)&lt;/a&gt;, or &lt;a href="https://kubernetes.io/docs/tasks/tools/#kubectl"&gt;kubectl&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; development environment. This article uses the &lt;a href="https://developers.redhat.com/articles/2021/12/14/explore-java-17-language-features-quarkus"&gt;Java 17&lt;/a&gt; version of the application, but any of the other three versions (natively-compiled Java 11, JVM Java 11, or natively-compiled Java 17) would work just the same.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Quarkus Superheroes sample application&lt;/h2&gt; &lt;p&gt;&lt;a href="products/quarkus/overview"&gt;Quarkus&lt;/a&gt; has excellent &lt;a href="https://quarkus.io/guides"&gt;documentation&lt;/a&gt; and &lt;a href="https://github.com/quarkusio/quarkus-quickstarts"&gt;quickstarts&lt;/a&gt; to help you become familiar with various features in the Quarkus ecosystem. However, what was missing before was a fully implemented sample set of real-world applications that use these features, patterns, and best practices while also reflecting the problems Quarkus is trying to solve.&lt;/p&gt; &lt;p&gt;Released in February 2022, the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;Quarkus Superheroes application&lt;/a&gt; consists of several microservices co-existing to form a more extensive system. Some microservices communicate synchronously via REST. Others are event-driven, producing and consuming events to and from &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt;. Some microservices are &lt;a href="https://www.reactivemanifesto.org"&gt;reactive&lt;/a&gt;, while others are traditional.&lt;/p&gt; &lt;p&gt;Figure 1 shows the overall architecture of the application.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_2.png?itok=jMAit2Kb" width="1440" height="1561" alt="The architecture of Quarkus Superheroes consists of elements of the game, plus services for a UI and statistics." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Overall architecture of the Quarkus Superheroes.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Detailed information about the application and its architecture can be found on the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue"&gt;quarkus.io blog&lt;/a&gt;. One of the &lt;a href="https://quarkus.io/blog/quarkus-superheroes-to-the-rescue/#requirements"&gt;main requirements&lt;/a&gt; when building the application was that it should be simple to deploy on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Let's test that theory.&lt;/p&gt; &lt;h2&gt;Deploy the application on the Developer Sandbox for Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; provides you with a private &lt;a href="products/openshift"&gt;Red Hat OpenShift&lt;/a&gt; environment, free for use for 30 days, in a shared, multi-tenant OpenShift cluster that is preconfigured with a set of developer tools. Your private OpenShift environment includes two projects (namespaces) and a resource quota of 7GB RAM and 15GB storage. Your application's development and stage phases can be emulated using the two namespaces. All user &lt;code&gt;Pod&lt;/code&gt;s are automatically deleted 12 hours after being created.&lt;/p&gt; &lt;h3&gt;Log into the Developer Sandbox&lt;/h3&gt; &lt;p&gt;You can spin up and access your Developer Sandbox with your Red Hat account. &lt;a href="https://redhat-scholars.github.io/managed-kafka-service-registry-workshop/managed-kafka-service-registry-workshop/main/03-quarkus-app-with-kafka-service-registry.html#devsandboxaccess"&gt;Follow these instructions&lt;/a&gt; to log into your Developer Sandbox account. Don't worry if you don't yet have a Red Hat account. The instructions will guide you through how to create and verify one.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You need to follow only the six steps in the &lt;strong&gt;Get access to the Developer Sandbox&lt;/strong&gt; section of the instructions.&lt;/p&gt; &lt;p&gt;You can move to the next section once you are in the Developer Perspective of your sandbox.&lt;/p&gt; &lt;h3&gt;Connect your local machine to the Developer Sandbox&lt;/h3&gt; &lt;p&gt;Now you need to connect your local machine to your sandbox. Follow &lt;a href="https://developers.redhat.com/blog/2021/04/21/access-your-developer-sandbox-for-red-hat-openshift-from-the-command-line"&gt;these instructions&lt;/a&gt; to download the &lt;a href="https://docs.openshift.com/container-platform/4.9/cli_reference/openshift_cli/getting-started-cli.html"&gt;OpenShift CLI&lt;/a&gt; (if you don't already have it) and run &lt;code&gt;oc login&lt;/code&gt; with the token from your sandbox. Once done, your terminal should be set in the &lt;code&gt;&lt;your-username&gt;-dev&lt;/code&gt; project.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;If you already have a Developer Sandbox account and have existing workloads in your project, you might need to delete those before deploying the Quarkus Superheroes application. The Developer Sandbox limits the resources each user can deploy at a single time.&lt;/p&gt; &lt;h3&gt;Deploy Quarkus Superheroes&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;&lt;a href="https://github.com/quarkusio/quarkus-super-heroes/tree/main/deploy/k8s"&gt;deploy/k8s&lt;/a&gt;&lt;/code&gt; directory in the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;root of the repository&lt;/a&gt; contains Kubernetes descriptors for each of the four versions of the application: JVM 11, JVM 17, natively compiled with Java 11, and natively compiled with Java 17.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Quarkus Superheroes repository contains Kubernetes descriptors for various flavors of Kubernetes: &lt;a href="https://www.openshift.com"&gt;OpenShift&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/deploying-to-kubernetes#deploying-to-minikube"&gt;minikube&lt;/a&gt;, &lt;a href="https://knative.dev"&gt;Knative&lt;/a&gt;, and "vanilla" &lt;a href="https://www.kubernetes.io"&gt;Kubernetes&lt;/a&gt;. The only real difference between the minikube and Kubernetes descriptors is that all the application &lt;code&gt;Service&lt;/code&gt;s in the minikube descriptors use &lt;code&gt;type: NodePort&lt;/code&gt;. A list of all the applications can be obtained simply by running &lt;code&gt;minikube service list&lt;/code&gt;. The Knative descriptors use &lt;a href="https://knative.dev/docs/serving"&gt;Knative Serving&lt;/a&gt; for each of the applications.&lt;/p&gt; &lt;p&gt;If you'd like, you can run &lt;code&gt;git clone&lt;/code&gt; to download the code from the &lt;a href="https://github.com/quarkusio/quarkus-super-heroes"&gt;Quarkus Superheroes GitHub repository&lt;/a&gt;. However, cloning isn't necessary because you can apply Kubernetes resources directly from remote locations.&lt;/p&gt; &lt;p&gt;Follow the following steps in your terminal to deploy the Java 17 version of the &lt;a href="https://quay.io/quarkus-super-heroes"&gt;application container images&lt;/a&gt;. Wait for each step to complete before proceeding with the next:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Deploy the application by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/java17-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy the &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt; monitoring service by executing: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f https://raw.githubusercontent.com/quarkusio/quarkus-super-heroes/main/deploy/k8s/prometheus-openshift.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;That's it—deploying the Superheroes is super simple! Once everything is deployed, your browser should look something like Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/deployed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/deployed.png?itok=24ZA2TGj" width="836" height="1160" alt="The topology for Quarkus Superheroes shows the relationships among the services." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Topology view for Quarkus Superheroes showing relationships among the services.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The application as deployed is &lt;em&gt;not&lt;/em&gt; considered production-ready. The databases, Prometheus instance, Kafka broker, and schema registry deployed are not highly available and do not use any Kubernetes operators for management or monitoring. They also use ephemeral storage.&lt;/p&gt; &lt;p&gt;Later in this article, we'll substitute a fully hosted and managed &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Kafka service&lt;/a&gt; and &lt;a href="https://console.redhat.com/application-services/service-registry"&gt;schema registry service&lt;/a&gt; into the mix.&lt;/p&gt; &lt;h2&gt;Interacting with the application&lt;/h2&gt; &lt;p&gt;Open the event statistics user interface (UI) by clicking the icon in the upper right corner of the &lt;code&gt;event-statistics&lt;/code&gt; application, shown in Figure 3.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opene.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opene.png?itok=EZZUt00h" width="345" height="304" alt="The topology for the event statistics UI provides a button to open it." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Button for opening the event-statistics UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once open, you should see the event-statistics UI shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/figure-4.png?itok=pPbIbTuU" width="913" height="246" alt="The event-statistics UI showing battle statistics" loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The event-statistics UI showing battle statistics.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There isn't anything here yet, but once we interact with the main application UI, there will be.&lt;/p&gt; &lt;p&gt;Similarly, open the Superheroes UI by clicking the icon in the upper right corner of the &lt;code&gt;ui-super-heroes&lt;/code&gt; application, shown in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opens.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opens.png?itok=gWclPICD" width="194" height="212" alt="The topology for the superheroes UI provides a button to open it." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Button for opening the Superheroes UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once open, you should see the Superheroes UI, shown in Figure 6. Highlighted in green in Figure 6 are clickable areas such as:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Expand/collapse the list of powers a hero or villain has&lt;/li&gt; &lt;li&gt;Randomly select a new hero and villain&lt;/li&gt; &lt;li&gt;Perform a battle&lt;/li&gt; &lt;/ul&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; You will most likely see different fighters than in the screenshot. They are randomly chosen.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/super.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/super.png?itok=-waXJzpI" width="600" height="490" alt="The Superheroes UI shows a villain and hero, randomly chosen." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Superheroes UI shows a villain and hero, randomly chosen.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Go ahead and perform a few battles, both with the same fighters and with new fighters. Once you've completed a few battles, note that a chronological list of battles is now displayed in the table on the screen.&lt;/p&gt; &lt;p&gt;You can also switch your browser tab back over to the event statistics UI. The slider in the event statistics UI should have moved one way or another or stayed in the middle if there were equal wins. There should also be a list of the top ten winners and the number of wins for each. Figure 7 shows an example.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/event-statistics-ui.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/event-statistics-ui.png?itok=FgsRS-EX" width="908" height="293" alt="event-statistics UI after a few battles." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: event-statistics UI after a few battles.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Messages in &lt;a href="https://quarkus.io/guides/kafka-schema-registry-avro"&gt;Apache Avro&lt;/a&gt; format arrive from &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt; from the &lt;code&gt;rest-fights&lt;/code&gt; service to the &lt;code&gt;event-statistics&lt;/code&gt; service. The schema for these messages is registered in an &lt;a href="https://www.apicur.io/registry"&gt;Apicurio Schema Registry&lt;/a&gt;. You can open the Apicurio Schema Registry by clicking the icon in the upper right corner of the &lt;code&gt;apicurio&lt;/code&gt; application, shown in Figure 8.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/opena.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/opena.png?itok=2HgygL5z" width="182" height="167" alt="The topology for the Apicurio UI provides a button to open it." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Button for opening the Apicurio UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once open, you should see the Apicurio UI, shown in Figure 9.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/apicurio.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/apicurio.png?itok=oqVaIUE-" width="581" height="347" alt="The Apicurio service registry instance contains a Fight instance." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: The Apicurio Service Registry instance containing a Fight schema.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Click the &lt;code&gt;Fight (fights-value)&lt;/code&gt; link to see all the details of the schema, including the Avro source of the schema itself.&lt;/p&gt; &lt;h2&gt;Creating managed services&lt;/h2&gt; &lt;p&gt;We already mentioned that the current setup is not production-ready. Furthermore, if the Kafka or Schema Registry &lt;code&gt;Pod&lt;/code&gt;s restart, all the data is lost. One way to fix this is to use fully hosted and managed Kafka and Schema Registry services.&lt;/p&gt; &lt;p&gt;Red Hat provides managed cloud services, known as the &lt;a href="https://console.redhat.com/application-services/overview"&gt;Red Hat OpenShift Application Services&lt;/a&gt;. We will use a free trial for two of these services, &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt;, to provide a set of production-ready services for our application.&lt;/p&gt; &lt;h3&gt;&lt;img alt="Anchor" src="image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" /&gt;Creating service accounts&lt;/h3&gt; &lt;p&gt;The first thing you need to do is create a few service accounts to be used by your applications. Two applications communicate with Kafka in this architecture: The &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services. Each service needs an individual service account for communicating with the managed services.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; In some instances, it might make sense for each service to have an individual service account for each managed service. In this article, we decided to keep things simple, so each service has a single service account for both managed services.&lt;/p&gt; &lt;p&gt;Follow these steps to create the two service accounts:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Open a new browser tab to the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;. The dashboard uses the Red Hat account with which you logged into the sandbox. You might need to accept some additional terms and conditions if this is the first time you've visited the dashboard.&lt;/li&gt; &lt;li&gt;On the left-hand navigation, select &lt;strong&gt;Service Accounts&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the middle of the screen, click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Short description&lt;/strong&gt; field, enter &lt;code&gt;event-statistics&lt;/code&gt; and click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Credentials successfully generated&lt;/strong&gt; screen is now shown. Save the &lt;strong&gt;Client ID&lt;/strong&gt; and &lt;strong&gt;Client secret&lt;/strong&gt; somewhere in a safe place. You will need them later, and they cannot be viewed again. These are the ID and secret for the &lt;code&gt;event-statistics&lt;/code&gt; service.&lt;/li&gt; &lt;li&gt;Once copied, check the &lt;strong&gt;I have copied the client ID and secret box&lt;/strong&gt; and click &lt;strong&gt;Close&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Service Accounts&lt;/strong&gt; screen will be shown. Click &lt;strong&gt;Create service account&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 4-6, but use &lt;code&gt;rest-fights&lt;/code&gt; as the &lt;strong&gt;Short Description&lt;/strong&gt; and copy the generated ID and secret for the &lt;code&gt;rest-fights&lt;/code&gt; service somewhere safe.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create the Service Registry service&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt; is based on the open-source &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry project&lt;/a&gt;. The service provides a highly available service registry instance that is secure and compatible with the &lt;a href="https://docs.confluent.io/platform/current/schema-registry/develop/api.html"&gt;Confluent Schema Registry API&lt;/a&gt; and &lt;a href="https://github.com/cloudevents/spec/blob/main/schemaregistry/spec.md"&gt;CNCF Schema Registry API&lt;/a&gt;. OpenShift Service Registry is also a perfect companion service for applications that use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; &lt;h4&gt;Create the Service Registry instance&lt;/h4&gt; &lt;p&gt;Follow these steps to create a Service Registry instance and give the service accounts the proper access:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;From the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;, expand the &lt;strong&gt;Service Registry&lt;/strong&gt; entry on the left-side menu and click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Service Registry Instance&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Name&lt;/strong&gt; field, enter &lt;code&gt;quarkus-superheroes&lt;/code&gt; and click &lt;strong&gt;Create&lt;/strong&gt;. &lt;ul&gt;&lt;li&gt;It might take a minute or so for the instance to create. The &lt;strong&gt;Status&lt;/strong&gt; column will display a green checkmark to indicate when the instance is ready for use, as shown in Figure 10.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/qs.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/qs.png?itok=0Y432kTj" width="1440" height="430" alt="The service registry shows that the quarkus-superheroes instance is ready." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The Apicurio Service Registry showing the quarkus-superheroes instance is ready.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Assign service account access to the Service Registry&lt;/h4&gt; &lt;p&gt;Now follow these steps to give the two service accounts access to the instance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the quarkus-superheroes instance.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; tab at the top.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Grant access&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Grant access&lt;/strong&gt; dialog that pops up, choose the service account for the &lt;code&gt;event-statistics&lt;/code&gt; service, choose the &lt;strong&gt;Manager&lt;/strong&gt; role, and click &lt;strong&gt;Save&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 3 and 4 to grant the &lt;strong&gt;Manager&lt;/strong&gt; role to the &lt;code&gt;rest-fights&lt;/code&gt; service account.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; We are giving the service accounts the &lt;strong&gt;Manager&lt;/strong&gt; role instead of the &lt;strong&gt;Viewer&lt;/strong&gt; role because both the &lt;code&gt;rest-fights&lt;/code&gt; and &lt;code&gt;event-statistics&lt;/code&gt; services publish the schema to the registry if it does not already exist.&lt;/p&gt; &lt;h4&gt;&lt;img alt="Anchor" src="image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" /&gt;Get Service Registry connection details&lt;/h4&gt; &lt;p&gt;Now follow these steps to get the connection details for the Service Registry instance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link on the left-side menu to return to the list of Service Registry instances.&lt;/li&gt; &lt;li&gt;Click the menu on the right of the &lt;code&gt;quarkus-superheros&lt;/code&gt; instance and select &lt;strong&gt;Connection&lt;/strong&gt;, as shown in Figure 11. The screen displays the client connection information for the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/connection_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/connection_0.png?itok=UKHSCSm3" width="600" height="185" alt="The line for an instance includes a Connection button at the right." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11: Get the Service Registry instance connection info.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Copy to clipboard&lt;/strong&gt; buttons for the &lt;strong&gt;Core Registry API&lt;/strong&gt; and the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; fields as shown in Figure 12. Save these URLs in a safe place—they will be needed later when you configure the applications. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/buttons.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/buttons.png?itok=6mfydiBn" width="600" height="837" alt="The web page for an instance includes buttons for Core Registry API and Token endpoint URL." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: The Service Registry instance connection details.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;OpenShift Service Registry provides the following APIs to connect applications to the service:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;strong&gt;Core Registry API&lt;/strong&gt; is the most powerful and works with Apicurio client libraries. This is the endpoint that the applications will connect to.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;Schema Registry compatibility API&lt;/strong&gt; provides compatibility with the Confluent Schema Registry API.&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;CNCF Schema Registry API&lt;/strong&gt; provides compatibility with the CNCF specification.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Creating an Apache Kafka instance&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; is a managed cloud service that provides a streamlined developer experience for building, deploying, and scaling new &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps"&gt;cloud-native applications&lt;/a&gt; or modernizing existing systems.&lt;/p&gt; &lt;h4&gt;Create a Kafka instance&lt;/h4&gt; &lt;p&gt;Follow these steps to create a Kafka instance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;From the &lt;a href="https://console.redhat.com/application-services"&gt;Red Hat OpenShift Application Services dashboard&lt;/a&gt;, expand the &lt;strong&gt;Streams for Apache Kafka&lt;/strong&gt; entry on the left-side menu, and click &lt;strong&gt;Kafka Instances&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Kafka Instance&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Name&lt;/strong&gt; field, enter &lt;code&gt;quarkus-superheroes&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Cloud region&lt;/strong&gt; field, select &lt;strong&gt;US East, N. Virginia&lt;/strong&gt;. Trial instances are not available in the EU region.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Click the &lt;strong&gt;Create Instance&lt;/strong&gt; button.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There is limited capacity in the service's free tier. If you get a message saying "Something went wrong or There was a problem processing the request," try again later.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;It might take a few minutes for the instance to create. The &lt;strong&gt;Status&lt;/strong&gt; column will display a green checkmark to indicate when the instance is ready for use, as shown in Figure 13.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ready.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ready.png?itok=4jAS6i4O" width="1440" height="472" alt="Kafka Instances page shows when an instance is ready in the Status field." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13: The Kafka instances showing the quarkus-superheroes instance is ready.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;Create a Kafka topic&lt;/h4&gt; &lt;p&gt;Now follow these steps to create a topic within the instance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topics&lt;/strong&gt; item on the top of the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance dashboard.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create topic&lt;/strong&gt; button in the middle of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Topic name&lt;/strong&gt; field, enter &lt;code&gt;fights&lt;/code&gt;, then click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Partitions&lt;/strong&gt; field, leave the selection of 1 in place and click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Message retention&lt;/strong&gt; screen, leave the defaults in place and click the &lt;strong&gt;Next&lt;/strong&gt; button at the bottom of the screen.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Replicas&lt;/strong&gt; screen, leave the defaults in place and click the &lt;strong&gt;Finish&lt;/strong&gt; button at the bottom of the screen. The topic will be created.&lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Assign service account access to Kafka&lt;/h4&gt; &lt;p&gt;Now follow these steps to assign proper access to the topic for the two service accounts:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;Access&lt;/strong&gt; item on the top of the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance dashboard.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Manage access&lt;/strong&gt; button at the top of the &lt;strong&gt;Access&lt;/strong&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Manage access&lt;/strong&gt; pop-up dialog that appears, select the &lt;code&gt;event-statistics&lt;/code&gt; service account from the &lt;strong&gt;Account&lt;/strong&gt; dropdown, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the down arrow next to the &lt;strong&gt;Add permission&lt;/strong&gt; button at the bottom of the pop-up dialog. Select &lt;strong&gt;Consume from a topic&lt;/strong&gt;, as shown in Figure 14, then click the &lt;strong&gt;Save&lt;/strong&gt; button at the bottom of the dialog. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/consume.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/consume.png?itok=LES3Fcnf" width="600" height="535" alt="The Manage Access screen lets a service consume from a Kafka topic." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 14: Access management for consuming messages from a Kafka topic.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Manage access&lt;/strong&gt; button at the top of the &lt;strong&gt;Access&lt;/strong&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Manage access&lt;/strong&gt; pop-up dialog that appears, select the &lt;code&gt;rest-fights&lt;/code&gt; service account from the &lt;strong&gt;Account&lt;/strong&gt; dropdown, then click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the down arrow next to the &lt;strong&gt;Add permission&lt;/strong&gt; button at the bottom of the pop-up dialog. Select &lt;strong&gt;Produce to a topic&lt;/strong&gt;, as shown in Figure 15, then click the &lt;strong&gt;Save&lt;/strong&gt; button at the bottom of the dialog. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/produce.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/produce.png?itok=zdPHNZrv" width="600" height="506" alt="The Manage Access screen lets a service produce to a Kafka topic." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 15: Access management for producing messages to a Kafka topic.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 16 shows the &lt;strong&gt;Access&lt;/strong&gt; tab containing all the Kafka permissions.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/permissions_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/permissions_0.png?itok=nxyQV5mo" width="1371" height="843" alt="The Manage Access screen shows all Kafka permissions." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 16: Access management for all Kafka permissions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;&lt;img alt="Anchor" src="image/gif;base64,R0lGODlhAQABAPABAP///wAAACH5BAEKAAAALAAAAAABAAEAAAICRAEAOw==" /&gt;Get Kafka connection details&lt;/h4&gt; &lt;p&gt;Now follow these steps to get the connection details for the Kafka instance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;Kafka Instances&lt;/strong&gt; link on the left-side menu to return to the list of Kafka instances.&lt;/li&gt; &lt;li&gt;Click the menu on the right of the &lt;code&gt;quarkus-superheros&lt;/code&gt; instance and select &lt;strong&gt;Connection&lt;/strong&gt;, as shown in Figure 17. This displays the client connection information for the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/info.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/info.png?itok=fv-1z74r" width="600" height="204" alt="The line for an instance includes a Connection button that can display client connection information." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 17: Get the Kafka instance connection info.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Copy to clipboard&lt;/strong&gt; buttons for the &lt;strong&gt;Bootstrap server&lt;/strong&gt; and the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; fields, as shown in Figure 18. Save these URLs in a safe place, because they will be needed later when you configure the applications.&lt;/li&gt; &lt;/ol&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;strong&gt;Token endpoint URL&lt;/strong&gt; is most likely the same as the service registry &lt;strong&gt;Token endpoint URL&lt;/strong&gt;. Both services use the same identity provider.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/boot.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/boot.png?itok=BrmSAHja" width="676" height="887" alt="The web page for an instance includes buttons for Bootstrap server and Token endpoint URL." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 18: The Kafka instance connection details.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Connect the application to managed services&lt;/h2&gt; &lt;p&gt;Now, let's connect our applications to the managed services we've created. First, let's get a little disruptive by deleting the existing Apicurio Schema Registry and Kafka workloads.&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Return to the Topology view of the Developer Sandbox, find the &lt;code&gt;apicurio&lt;/code&gt; workload, right-click it, and select &lt;strong&gt;Delete Deployment&lt;/strong&gt; as shown in Figure 19. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/delete.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/delete.png?itok=gqNj1lig" width="340" height="567" alt="The topology for an instance lets you delete the deployment." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 19: Delete Apicurio Deployment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;When the &lt;strong&gt;Delete Deployment&lt;/strong&gt; confirmation pops up, check the &lt;strong&gt;Delete dependent objects of this resource&lt;/strong&gt; box and click &lt;strong&gt;Delete&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Repeat steps 1 and 2 to delete the &lt;code&gt;fights-kafka&lt;/code&gt; workload as well.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Deleting these workloads might cause some of the other workloads to turn red or show errors in their logs, which is expected. Let's now fix the applications by connecting them to the managed services.&lt;/p&gt; &lt;p&gt;Every application has a corresponding &lt;code&gt;ConfigMap&lt;/code&gt; and &lt;code&gt;Secret&lt;/code&gt; containing its configuration and credentials. These two places in the &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services need to be updated with the new configuration.&lt;/p&gt; &lt;h3&gt;Connect the event-statistics service&lt;/h3&gt; &lt;p&gt;Follow these steps to connect the &lt;code&gt;event-statistics&lt;/code&gt; service to the managed services:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;ConfigMaps&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;event-statistics-config&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;event-statistics-config&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;code&gt;data&lt;/code&gt; section near the bottom, perform the following: &lt;ul&gt;&lt;li&gt;Replace the value of &lt;code&gt;kafka.bootstrap.servers&lt;/code&gt; with the &lt;strong&gt;Bootstrap server&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetKafkaConnectionDetails"&gt;Getting Kafka connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace the value of &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.registry.url&lt;/code&gt; with the &lt;strong&gt;Core Registry API&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Getting service registry connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Add the following new key/value combinations:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.auth.service.token.endpoint: &lt;Token endpoint URL&gt; mp.messaging.connector.smallrye-kafka.sasl.jaas.config: org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="&lt;Token endpoint URL&gt;" ; mp.messaging.connector.smallrye-kafka.security.protocol: SASL_SSL mp.messaging.connector.smallrye-kafka.sasl.mechanism: OAUTHBEARER mp.messaging.connector.smallrye-kafka.sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler mp.messaging.connector.smallrye-kafka.apicurio.auth.client.id: ${CLIENT_ID} mp.messaging.connector.smallrye-kafka.apicurio.auth.client.secret: ${CLIENT_SECRET}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;Token endpoint URL&gt;&lt;/code&gt; with the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; value you saved earlier from either the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Getting service registry connection details&lt;/a&gt; or &lt;a href="#GetKafkaConnectionDetails"&gt;Getting Kafka connection details&lt;/a&gt; section. The value should be the same in both places.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When complete, the data section in your &lt;code&gt;event-statistics-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; should look something like Figure 20.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/configmap_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/configmap_0.png?itok=Sxy29CwG" width="600" height="97" alt="The YAML in the event-statistics-config ConfigMap is complete." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 20: The event-statistics-config ConfigMap YAML contents.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Secrets&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;event-statistics-config-creds&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;event-statistics-config-creds&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;Add a &lt;code&gt;stringData&lt;/code&gt; element at the bottom and add the following key/value pairs, as shown in Figure 21: &lt;pre&gt; &lt;code&gt;CLIENT_ID: &lt;event-statistics-client-id&gt; CLIENT_SECRET: &lt;event-statistics-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Replace &lt;code&gt;&lt;event-statistics-client-id&gt;&lt;/code&gt; with the client ID for the &lt;code&gt;event-statistics&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;event-statistics-client-secret&gt;&lt;/code&gt; with the client secret for the &lt;code&gt;event-statistics&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/creds.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/creds.png?itok=qHPrpmXk" width="451" height="57" alt="The YAML for the event-statistics-config-creds shows CLIENT_ID and CLIENT_SECRET." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 21: The event-statistics-config-creds Secret YAML contents.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topology&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and right-click the &lt;code&gt;event-statistics&lt;/code&gt; workload, then select &lt;strong&gt;Start rollout&lt;/strong&gt; as shown in Figure 22. A new instance of the &lt;code&gt;event-statistics&lt;/code&gt; workload will be rolled out with the updated configuration. The rollout should only take a minute or less.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/roll.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/roll.png?itok=tw-07o1x" width="460" height="656" alt="The topology for event-statistics allows you to start the rollout." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 22: Start a rollout of the event-statistics service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Move on and connect the &lt;code&gt;rest-fights&lt;/code&gt; service while the &lt;code&gt;event-statistics&lt;/code&gt; is rolling out.&lt;/p&gt; &lt;h3&gt;Connect the rest-fights service&lt;/h3&gt; &lt;p&gt;Next, follow these steps to connect the &lt;code&gt;rest-fights&lt;/code&gt; service to the managed services, which are nearly identical to the previous steps connecting the &lt;code&gt;event-statistics&lt;/code&gt; service:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click the &lt;strong&gt;ConfigMaps&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;rest-fights-config&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;rest-fights-config&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;In the &lt;code&gt;data&lt;/code&gt; section toward the bottom, perform the following: &lt;ul&gt;&lt;li&gt;Replace the value of &lt;code&gt;kafka.bootstrap.servers&lt;/code&gt; with the &lt;strong&gt;Bootstrap server&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetKafkaConnectionDetails"&gt;Get Kafka connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace the value of &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.registry.url&lt;/code&gt; with the &lt;strong&gt;Core Registry API&lt;/strong&gt; value you saved earlier from the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Get service registry connection details&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt; &lt;p&gt;Add the following new key/value combinations:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;mp.messaging.connector.smallrye-kafka.apicurio.auth.service.token.endpoint: &lt;Token endpoint URL&gt; mp.messaging.connector.smallrye-kafka.sasl.jaas.config: org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required oauth.client.id="${CLIENT_ID}" oauth.client.secret="${CLIENT_SECRET}" oauth.token.endpoint.uri="&lt;Token endpoint URL&gt;" ; mp.messaging.connector.smallrye-kafka.security.protocol: SASL_SSL mp.messaging.connector.smallrye-kafka.sasl.mechanism: OAUTHBEARER mp.messaging.connector.smallrye-kafka.sasl.login.callback.handler.class: io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler mp.messaging.connector.smallrye-kafka.apicurio.auth.client.id: ${CLIENT_ID} mp.messaging.connector.smallrye-kafka.apicurio.auth.client.secret: ${CLIENT_SECRET}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;Token endpoint URL&gt;&lt;/code&gt; with the &lt;strong&gt;Token endpoint URL&lt;/strong&gt; value you saved earlier from either the &lt;a href="#GetServiceRegistryConnectionDetails"&gt;Get service registry connection details&lt;/a&gt; or &lt;a href="#GetKafkaConnectionDetails"&gt;Get Kafka connection details&lt;/a&gt; section. The value should be the same in both.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;When complete, the data section in your &lt;code&gt;rest-fights-config&lt;/code&gt; &lt;code&gt;ConfigMap&lt;/code&gt; should look something like Figure 23.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rest.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rest.png?itok=0OcsKLsv" width="600" height="145" alt="The YAML in the rest-fights-config ConfigMap is complete." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 23: The rest-fights-config ConfigMap YAML contents.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Secrets&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and click the &lt;code&gt;rest-fights-config-creds&lt;/code&gt; item.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;YAML&lt;/strong&gt; tab at the top of the &lt;code&gt;rest-fights-config-creds&lt;/code&gt; screen.&lt;/li&gt; &lt;li&gt;Add a &lt;code&gt;stringData&lt;/code&gt; element at the bottom and add the following key/value pairs, as shown in Figure 24: &lt;pre&gt; &lt;code&gt;CLIENT_ID: &lt;rest-fights-client-id&gt; CLIENT_SECRET: &lt;rest-fights-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;ul class="Indent1"&gt;&lt;li&gt;Replace &lt;code&gt;&lt;rest-fights-client-id&gt;&lt;/code&gt; with the client ID for the &lt;code&gt;rest-fights&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section.&lt;/li&gt; &lt;li&gt;Replace &lt;code&gt;&lt;rest-fights-client-secret&gt;&lt;/code&gt; with the client secret for the &lt;code&gt;rest-fights&lt;/code&gt; service you created in the &lt;a href="#CreateServiceAccounts"&gt;Creating service accounts&lt;/a&gt; section. &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/yc.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/yc.png?itok=tbF2E7-G" width="456" height="63" alt="The YAML for the rest-fights-config-creds shows CLIENT_ID and CLIENT_SECRET." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 24: The rest-fights-config-creds Secret YAML contents.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Save&lt;/strong&gt; at the bottom of the screen.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Topology&lt;/strong&gt; link on the left-hand navigation of the sandbox.&lt;/li&gt; &lt;li&gt;Find and right-click the &lt;code&gt;rest-fights&lt;/code&gt; workload, then select &lt;strong&gt;Start rollout&lt;/strong&gt; as shown in Figure 25. A new instance of the &lt;code&gt;rest-fights&lt;/code&gt; workload will be rolled out with the updated configuration. The rollout should take a minute or less.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rroll.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rroll.png?itok=DRCyonD-" width="384" height="584" alt="The topology for rest-fights allows you to start the rollout." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 25: Start a rollout of the rest-fights service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Re-examine the application&lt;/h2&gt; &lt;p&gt;The application is ready again when the &lt;code&gt;event-statistics&lt;/code&gt; and &lt;code&gt;rest-fights&lt;/code&gt; services have solid blue circles around their icons. Once these services are ready, go back to the main Superheroes and event statistics UIs and refresh the browser. The application should function just as before, except now the messages are being sent to a managed Kafka instance, and the schema is stored in a managed Service Registry instance.&lt;/p&gt; &lt;p&gt;Go to the &lt;a href="https://console.redhat.com/application-services/service-registry"&gt;&lt;strong&gt;Service Registry Instances&lt;/strong&gt; dashboard&lt;/a&gt; and click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. You should see the same &lt;code&gt;Fight&lt;/code&gt; schema as you did before with the self-deployed Apicurio instance.&lt;/p&gt; &lt;p&gt;Next, go to the &lt;a href="https://console.redhat.com/application-services/streams/kafkas"&gt;Kafka Instances dashboard&lt;/a&gt; and click the &lt;code&gt;quarkus-superheroes&lt;/code&gt; instance. You should see a bunch of metrics around the instance, topics, client connections, message rates, and more.&lt;/p&gt; &lt;h2&gt;What's next?&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Application Services delivers a streamlined developer experience for building, deploying, and scaling cloud-native applications.&lt;/p&gt; &lt;p&gt;Don't miss your chance to preview some of our new application services. Want to bring some artificial intelligence and machine learning into your applications? Want to put an API management layer in front of all your services? If so, take a look at some of the other managed &lt;a href="https://console.redhat.com/application-services/overview"&gt;OpenShift Application Services&lt;/a&gt;, such as &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science"&gt;Red Hat OpenShift Data Science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/rhoam/getting-started"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/23/quarkus-superheroes-managed-services-save-day" title="Quarkus Superheroes: Managed services save the day"&gt;Quarkus Superheroes: Managed services save the day&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2022-03-23T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 17.0.1 released</title><link rel="alternate" href="https://www.keycloak.org/2022/03/keycloak-1701-released" /><author><name /></author><id>https://www.keycloak.org/2022/03/keycloak-1701-released</id><updated>2022-03-23T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 16.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES NEW FEATURES * Add Reactive Keycloak-X Admin Client keycloak ENHANCEMENTS * Multi-arch container image for Quarkus distribution (amd64 + arm64) keycloak dist/quarkus * Removing unnecessary code paths during startup keycloak * Change DOCTYPE for base theme to keycloak * Add ability to control debug suspend mode to kc scripts keycloak * Updating proxy guide with x509 client certificate lookup keycloak docs * Configuring providers guide keycloak * Update Quarkus to 2.7.4 keycloak dist/quarkus * Update Quarkus to 2.7.5 keycloak dist/quarkus BUGS * Favicon missing in welcome page on chrome / macos keycloak core * Different behavior between Quarkus and Wildfly distribution: SQLException on POST returns HTTP 201 instead 500 keycloak dist/quarkus * Quarkus-based distribution cannot use MariaDB with Galera replication due to XA transactions being unsupported keycloak dist/quarkus * start-up with kc.bat outputs an error if folder name includes space keycloak * On SQLException, invalid Content-Length header in response keycloak dist/quarkus * Wrong description for import/export options keycloak dist/quarkus * Change the flush mode to auto keycloak dist/quarkus * Can't change http-port by Keycloak/Quarkus on Windows keycloak dist/quarkus * Keycloak admin UI drops https port if --hostname is used keycloak dist/quarkus * kc.sh build option "--help-all" not found keycloak dist/quarkus * Update to Liquibase 4.8.0 keycloak storage * Capacity to change hibernate dialect keycloak dist/quarkus UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title>What’s new for developers in the OpenShift 4.10 console</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/22/whats-new-developers-openshift-410-console" /><author><name>Serena Chechile Nichols</name></author><id>c7dc78fe-0fd6-41c4-a62b-a646da6dbba2</id><updated>2022-03-22T17:00:00Z</updated><published>2022-03-22T17:00:00Z</published><summary type="html">&lt;p&gt;This article covers what’s new for developers in the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; console in OpenShift 4.10. This release includes many usability improvements, including changing your defaults for routes in creation flows and the ability to quickly troubleshoot misbehaving pods from the user interface (UI).&lt;/p&gt; &lt;p&gt;Additionally, there are new features available in the console when you install the following operators on your cluster: &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/intro.html"&gt;Service Binding Operator&lt;/a&gt;, &lt;a href="https://github.com/konveyor/gitops-primer"&gt;gitops-primer&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Red Hat OpenShift Serverless&lt;/a&gt;, &lt;a href="https://developers.redhat.com/courses/middleware/openshift-pipelines"&gt;Red Hat OpenShift Pipelines&lt;/a&gt;, and &lt;a href="https://docs.openshift.com/container-platform/4.8/cicd/gitops/understanding-openshift-gitops.html"&gt;Red Hat OpenShift GitOps&lt;/a&gt;. Let’s dive into the details.&lt;/p&gt; &lt;h2&gt;Secure routes by default&lt;/h2&gt; &lt;p&gt;In 4.10, we now default to secure routes in our &lt;strong&gt;Import from Git and Deploy Image&lt;/strong&gt; flows. In the detailed settings, you'll find secure routes with Transport Layer Security (TLS) termination set to &lt;strong&gt;Edge&lt;/strong&gt; and Insecure traffic set to &lt;strong&gt;Redirect&lt;/strong&gt;, as shown in Figure 1.&lt;/p&gt; &lt;p&gt;If these settings aren’t what you are looking for, not to worry. We've enhanced the User Preferences to include an Application tab that lets you set all of the routing options that are used as defaults for routing options in &lt;strong&gt;Deploy Image &amp; Import from Git&lt;/strong&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="User Preferences - Default route settings" data-entity-type="file" data-entity-uuid="0a72da8f-2cdf-4777-92e0-1f8f907fafa3" src="https://developers.redhat.com/sites/default/files/inline-images/410Blog-UserPrefs-Apps.png" width="1535" height="970" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: The new default route settings under User Preferences.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Quick search from Add&lt;/h2&gt; &lt;p&gt;In the 4.7 release, we added a quick search feature to &lt;strong&gt;Topology&lt;/strong&gt;. As of version 4.10, this feature is now available on the &lt;strong&gt;Add&lt;/strong&gt; page (see Figure 2). You can quickly search for an item available in the developer catalog or quick starts catalog, rather than having to drill into the catalogs to locate it.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Quick search from Add page" data-entity-type="file" data-entity-uuid="f2b38be2-d6a2-4ade-9ffb-95f8fab66552" src="https://developers.redhat.com/sites/default/files/inline-images/410Blog-AddQuickSearch.png" width="1565" height="920" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quick search from Add page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Topology&lt;/h2&gt; &lt;p&gt;The &lt;strong&gt;Topology&lt;/strong&gt; page (Figure 3) features several usability enhancements in OpenShift 4.10. Highlights include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Users can toggle between filtering by label or by name.&lt;/li&gt; &lt;li&gt;We now have a resizable side panel.&lt;/li&gt; &lt;li&gt;The side panel width is remembered per user.&lt;/li&gt; &lt;li&gt;The last selected item is remembered per user, per project, and per session.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can see these Topology enhancements in action in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Topology enhancements" data-entity-type="file" data-entity-uuid="86dc02c0-fbfe-49a0-8c3d-3d402f481606" src="https://developers.redhat.com/sites/default/files/inline-images/410Blog-Topology.png" width="1565" height="919" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 3: New usability enhancements are shown in the Topology view.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Helm&lt;/h2&gt; &lt;p&gt;You can now add additional Helm charts into the Helm Catalog of a specific project using the &lt;code&gt;ProjectHelmChartRepository&lt;/code&gt; custom resource (CR). We've also added a quick start to guide developers through the process of creating a project-scoped Helm chart repository. This quick start is available from the Helm chart catalog description text as well as in the quick start catalog.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Developers will need additional privileges to create namespace-scoped Helm repositories. Learn more by reading the &lt;a href="https://docs.openshift.com/container-platform/4.10/applications/working_with_helm_charts/configuring-custom-helm-chart-repositories.html#adding-namespace-scoped-helm-chart-repositories.adoc_configuring-custom-helm-chart-repositories"&gt;OpenShift documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This feature will be available in Red Hat OpenShift Container Platform 4.10.1.&lt;/p&gt; &lt;h2&gt;New service binding features in the Developer Perspective&lt;/h2&gt; &lt;p&gt;The &lt;strong&gt;Create Service Binding&lt;/strong&gt; action now lets you create service bindings in addition to dragging. With the Create Service Binding action, all you have to do is enter a name for the service binding and select the bindable service in your current project to connect to. When dragging the connector handle from the deployment workload and dropping it on the bindable service, simply enter a name for the service binding.&lt;/p&gt; &lt;p&gt;You can access these features when the Red Hat OpenShift Service Binding operator is installed.&lt;/p&gt; &lt;h2&gt;Export Application usability improvements&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Export Application &lt;/strong&gt;(available on the Topology page) has a few usability improvements with this release (Figure 4). When using this feature, you will see a &lt;strong&gt;View Logs&lt;/strong&gt; link in both the toast notification and the &lt;strong&gt;Export Application&lt;/strong&gt; modal so you can watch the logs of the job performing the export.&lt;/p&gt; &lt;p&gt;You can try this feature now on the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; or access it on your cluster when the &lt;strong&gt;gitops-primer&lt;/strong&gt; operator is installed.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Export application" data-entity-type="file" data-entity-uuid="35e77df7-da6d-453d-a564-546858ba4b16" src="https://developers.redhat.com/sites/default/files/inline-images/410Blog-Export-Viewlog.png" width="3584" height="2240" loading="lazy" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 4: Export application improvements.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;OpenShift 4.10 Serverless and Eventing features&lt;/h2&gt; &lt;p&gt;There are a number of new Eventing and Serverless features to note in OpenShift 4.10:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The Channel creation flow is now consistent between form-based creation and the YAML editor.&lt;/li&gt; &lt;li&gt;Kubernetes services are now available as valid sink resources during Event Source creation when adding a Trigger to a Broker and when adding a Subscription to a Channel.&lt;/li&gt; &lt;li&gt;Although the console does not yet support the creation of event sinks, &lt;strong&gt;Topology&lt;/strong&gt; will now visualize event sinks.&lt;/li&gt; &lt;li&gt;Knative services observability is here! Two serverless-related dashboards are now available in the &lt;strong&gt;Dashboard&lt;/strong&gt; tab of the &lt;strong&gt;Observe&lt;/strong&gt; page in the Developer perspective. Click on the Dashboard dropdown and you will see the following options: &lt;ul&gt;&lt;li&gt;Knative Serving - Revision CPU&lt;/li&gt; &lt;li&gt;Memory and Network Usage&lt;/li&gt; &lt;li&gt;Knative Serving - Revision Queue Proxy Metrics&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;You can now see the support level of an event source (Community Kamelets, too) in the side panel after selecting it from the catalog. The support level (Tech Preview, Supported, or Community) is indicated in the Support section.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can try these features now on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; or access them on your cluster when the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html/serverless/install#install-serverless-operator"&gt;Red Hat OpenShift Serverless&lt;/a&gt; operator is installed.&lt;/p&gt; &lt;h2&gt;OpenShift Pipelines enhancements&lt;/h2&gt; &lt;p&gt;The &lt;strong&gt;Logs&lt;/strong&gt; tab of the &lt;strong&gt;Pipeline Run&lt;/strong&gt; details page now displays the log snippet of failed &lt;strong&gt;Pipeline Runs&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;A number of updates have been made to the Import from Git flow when opting into adding a pipeline:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The UI now allows the user to select from a list of pipelines during the Import from Git flow when there are multiple pipeline templates available per runtime.&lt;/li&gt; &lt;li&gt;Previously, webhooks had to be created manually. Now, when you add a pipeline created when importing applications from Git, it generates a &lt;code&gt;TektonTrigger&lt;/code&gt; resource. You need to copy the URL and go to Git to configure the repo appropriately.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Tekton Hub integration with the Pipeline Builder&lt;/h3&gt; &lt;p&gt;Don’t miss the updates to &lt;a href="https://hub.tekton.dev/"&gt;Tekton Hub&lt;/a&gt; integration with the Pipeline Builder:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The list of available tasks in the Pipeline Builder has been updated to only display Tekton Hub tasks that are supported by the platform’s architecture.&lt;/li&gt; &lt;li&gt;Users now have the ability to link directly to task documentation on hub.tekton.dev when available.&lt;/li&gt; &lt;li&gt;Cluster administrators can turn off integration with the Tekton Hub so that community tasks are not available through the Pipeline Builder.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can access these features when the Red Hat OpenShift Serverless 1.7 operator is installed.&lt;/p&gt; &lt;h2&gt;OpenShift GitOps enhancements&lt;/h2&gt; &lt;p&gt;The &lt;strong&gt;Application Environments&lt;/strong&gt; page now shows health status indicators for unhealthy resources in an application. To learn more, check out the article &lt;a href="https://developers.redhat.com/articles/2022/02/09/gitops-using-red-hat-openshift-console-49-and-410"&gt;GitOps using Red Hat OpenShift Console 4.9 and 4.10&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Dynamic plugins&lt;/h2&gt; &lt;p&gt;In OpenShift 4.10, Dynamic plugins are being released in tech preview. Dynamic plugins let you build advanced integrations native to the OpenShift console. They can augment existing perspectives, like the admin and developer perspectives, or even build their own perspective. Creators will have the ability to add their own navigation items, pages (list, details, dashboard, etc.), tabs, actions, and much, much more. Read &lt;a href="http://content.cloud.redhat.com/blog/introducing-red-hat-openshift-4.10"&gt;this article on the Red Hat hybrid cloud blog&lt;/a&gt; to learn more about dynamic plugins.&lt;/p&gt; &lt;h2&gt;Try OpenShift 4.10&lt;/h2&gt; &lt;p&gt;Ready to try these new features for yourself? &lt;a href="http://www.openshift.com/try"&gt;Get started with OpenShift 4.10 today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift developer experience, so we want to hear from you. &lt;a href="https://twitter.com/serenamarie125"&gt;Tweet me @serenamarie125&lt;/a&gt; or join the &lt;a href="https://groups.google.com/forum/#!forum/openshift-dev-users"&gt;OpenShift Developer Experience Google group&lt;/a&gt; to share your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/22/whats-new-developers-openshift-410-console" title="What’s new for developers in the OpenShift 4.10 console"&gt;What’s new for developers in the OpenShift 4.10 console&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Serena Chechile Nichols</dc:creator><dc:date>2022-03-22T17:00:00Z</dc:date></entry><entry><title>Write Kubernetes in Java with the Java Operator SDK, Part 2</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2" /><author><name>Christophe Laprun</name></author><id>a5f72d15-f7dc-4129-a9dd-fbc46277c218</id><updated>2022-03-22T07:00:00Z</updated><published>2022-03-22T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt;, or JOSDK, is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project that aims to simplify the task of creating &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Operators using &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. The project was started by &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt;, and Red Hat is now a major contributor.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk"&gt;Part 1 in this series&lt;/a&gt; introduced JOSDK and explained why it could be interesting to create Operators in Java. In this article and its sequels, you will take a deeper look at JOSDK's concepts and learn how it simplifies Operator development. Along the way, you'll build a simple example using JOSDK and its &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;quarkus-operator-sdk&lt;/a&gt; extension for &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;, a Kubernetes-native Java stack.&lt;/p&gt; &lt;h2&gt;Use case&lt;/h2&gt; &lt;p&gt;Deploying an application on Kubernetes requires creating multiple resources: you need to create a &lt;code&gt;Deployment&lt;/code&gt; and an associated &lt;code&gt;Service&lt;/code&gt; at the very least. If you intend to access your application from outside the cluster (that is, if it's not simply a service that is used by some bigger application), you will also need to create an &lt;code&gt;Ingress&lt;/code&gt; (or a &lt;code&gt;Route&lt;/code&gt; if you're targeting &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;While this is not too difficult to do manually, you may want to automate the process so that you can focus on developing your application instead of worrying about how to deploy it quickly to your cluster during the development phase. This article will show you one way to do that: by developing a Kubernetes extension in the form of an Operator that will take custom resources we'll call &lt;code&gt;ExposedApp&lt;/code&gt; (for &lt;em&gt;exposed application&lt;/em&gt;) and process them to expose applications via their Docker image reference. The Operator will take the image reference and create the associated &lt;code&gt;Deployment&lt;/code&gt;, &lt;code&gt;Service&lt;/code&gt;, and &lt;code&gt;Ingress&lt;/code&gt; for you. In the grand Kubernetes declarative tradition, all this happens without you having to worry about the details of how the Operator does it.&lt;/p&gt; &lt;p&gt;Of course, moving the application to production would be a different issue. But the goal of this article is to present a somewhat realistic scenario that will showcase JOSDK in a way that's not overly complex. The example will simplify the use case, at least initially, by not worrying about the port that the application exposes; instead, it will just target a simple &lt;code&gt;Hello World&lt;/code&gt; type of application. You could, however, use this simple idea as this first step in building a more robust and widely applicable Operator.&lt;/p&gt; &lt;p&gt;All that being said, take a look at the &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resource &lt;/a&gt; (CR) for the &lt;code&gt;ExposedApp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: "halkyon.io/v1alpha1" kind: ExposedApp metadata: name: &lt;Name of our application&gt; spec: imageRef: &lt;Docker image reference&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Remember that an Operator is the combination of an API/DSL (domain-specific language), represented by CRs and their associated custom resource definitions (CRDs), and a controller capable of handling that DSL to perform the required actions to materialize the desired state as described by the CR. In the example application, the controller will need to react to the creation, update, or deletion of &lt;code&gt;ExposedApp&lt;/code&gt; resources on the cluster.&lt;/p&gt; &lt;p&gt;In architectural terms, when using JOSDK you must create a Java class—modeled following conventions we will discuss later in the article—to represent the CR and then use that class to parameterize a &lt;code&gt;Reconciler&lt;/code&gt; interface implementation. When using the Quarkus extension for JOSDK, that's pretty much all you have to do to write a simple Operator.&lt;/p&gt; &lt;p&gt;JOSDK provides an &lt;code&gt;Operator&lt;/code&gt; class that manages all the controllers (usually one per CR type) that your Operator needs to provide. It also manages the infrastructure needed to propagate the low-level events sent by the Kubernetes API all the way up to the controller's appropriate method, taking care to deserialize the objects sent by the API to Java objects that the controller can readily consume. This allows you to focus on the logic of your controller. There is, in particular, no need to implement watchers or informers (or other low-level details you'd need to handle yourself when writing Operators in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; or other languages) for your custom resources. JOSDK provides other utilities, such as automatic retries on error or caching, so that querying the Kubernetes API is kept to a minimum.&lt;/p&gt; &lt;p&gt;When using the Quarkus extension for JOSDK, though, you rarely, if ever, need to interact with the &lt;code&gt;Operator&lt;/code&gt; object, as the extension takes care of some additional steps that would be required if you were not using it. It will, for example, automatically create the &lt;code&gt;Operator&lt;/code&gt; instance, register your controllers with it, and start the &lt;code&gt;Operator&lt;/code&gt; when ready so that your controller can start processing events. As you will see later, the Quarkus extension does a lot more, but this is already a nice perk!&lt;/p&gt; &lt;h2&gt;Custom resource model&lt;/h2&gt; &lt;p&gt;Let's look more closely at the details. One thing to be aware of is that JOSDK relies on the &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;Fabric8 Kubernetes client&lt;/a&gt; in the same way that the Go &lt;a href="https://github.com/kubernetes-sigs/controller-runtime"&gt;controller-runtime&lt;/a&gt; project relies on &lt;a href="https://github.com/kubernetes/client-go"&gt;client-go&lt;/a&gt;. The Fabric8 client provides fluent APIs to interact with Kubernetes resources as well as a wealth of extensions and utilities that make working with Kubernetes from Java a little easier. In particular, Fabric8 provides specific support to work with custom resources in the form of a &lt;code&gt;CustomResource&lt;/code&gt; class. This class enforces the good practice of separating the state of the CR into two fields: &lt;code&gt;spec&lt;/code&gt; and &lt;code&gt;status&lt;/code&gt;. The &lt;code&gt;spec&lt;/code&gt; field represents the desired state that the user wants to apply to the cluster. It is, therefore, under the user's control and should not be modified by other processes. The &lt;code&gt;status&lt;/code&gt; field, on the other hand, represents the information that the controller associated to the CR wants to surface to the user (or other controllers). It represents the cluster's actual state and is the controller's responsibility; users cannot change its values. Each field is modeled by a separate class within Fabric8, and the &lt;code&gt;CustomResource&lt;/code&gt; class reflects this dichotomy: it is parameterized by both classes.&lt;/p&gt; &lt;p&gt;Moreover, you need to annotate your CR class to specify at minimum the associated group and version that Kubernetes needs to expose the API endpoint to clients. This is done using the &lt;code&gt;@Group&lt;/code&gt; and &lt;code&gt;@Version&lt;/code&gt; annotations. More annotations are also available to specify more information in case what's automatically generated doesn't match your requirements. For example, the plural form and kind are automatically inferred from the class name if not explicitly provided.&lt;/p&gt; &lt;p&gt;Finally, you also need to specify whether your custom resource is cluster- or namespace-scoped. The Fabric8 Kubernetes client uses a marker interface &lt;code&gt;io.fabric8.kubernetes.api.model.Namespaced&lt;/code&gt; for that purpose. Classes implementing this interface will be namespace-scoped, while other classes will be marked as cluster-scoped.&lt;/p&gt; &lt;p&gt;In the sample application, the CR should be namespace-scoped. Here's what the &lt;code&gt;ExposedApp&lt;/code&gt; CR would look like, where &lt;code&gt;ExposedAppSpec&lt;/code&gt; and &lt;code&gt;ExposedAppStatus&lt;/code&gt; are the classes holding the &lt;code&gt;spec&lt;/code&gt; and &lt;code&gt;status&lt;/code&gt; state, respectively:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;&lt;br /&gt;@Version("v1alpha1") @Group("halkyon.io") public class ExposedApp extends CustomResource&lt;ExposedAppSpec, ExposedAppStatus&gt; implements Namespaced { } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Implement the controller&lt;/h2&gt; &lt;p&gt;Now that this is out of the way, you're ready to start implementing the controller. To accelerate your work, you will use the &lt;a href="https://github.com/operator-framework/operator-sdk"&gt;operator-sdk&lt;/a&gt; tool to generate a skeleton project using its &lt;a href="https://github.com/operator-framework/java-operator-plugins"&gt;Quarkus plug-in&lt;/a&gt;. Refer to the documentation on their respective sites to set up these tools for your environment. You will also need a working Java 11 development environment, including Maven 3.8.1+. Note that you will need to be connected to a Kubernetes cluster on which you have enough privileges to deploy custom resource definitions. As this series focuses on the experience of writing Operators using JOSDK and its Quarkus extension, we won't get into the details of what would be required to deploy your Operator to your cluster and will instead focus on the experience of developing your Operator locally.&lt;/p&gt; &lt;p&gt;Begin by creating the skeleton project in a new &lt;code&gt;exposedapp&lt;/code&gt; directory.&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;&gt; mkdir exposedapp &gt; cd exposedapp &gt; operator-sdk init --plugins quarkus --domain halkyon.io --project-name expose &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As mentioned previously, you need to specify that you want to initialize the project using the &lt;code&gt;quarkus&lt;/code&gt; plug-in. You should also specify a domain name, &lt;code&gt;halkyon.io&lt;/code&gt; in this particular instance, which will be associated with the group for your CR and serve as the package name for your Java classes.&lt;/p&gt; &lt;p&gt;This will create a directory structure similar to this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;. ├── Makefile ├── PROJECT ├── pom.xml └── src └── main ├── java └── resources └── application.properties 4 directories, 4 files &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is a fairly standard Java Maven project layout. You might have noticed, though, that no Java code has been generated so far. But because the project has been configured to use the Quarkus extension for JOSDK, you can start &lt;a href="https://quarkus.io/guides/maven-tooling#dev-mode"&gt;Quarkus dev mode&lt;/a&gt; so that you can begin live-coding your controller.&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;mvn quarkus:dev &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, you can't expect your Operator to do much: after all, you haven't written the controller (or any other Java code, for that matter). The Quarkus extension for JOSDK, however, sets things up for you and lets you know that you have more work to do, as evidenced by the error message on the console:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;WARN [io.qua.ope.run.AppEventListener] (Quarkus Main Thread) No Reconciler implementation was found so the Operator was not started. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step, then, is to add a controller and its associated CR.&lt;/p&gt; &lt;h3&gt;Iterative custom resource implementation&lt;/h3&gt; &lt;p&gt;As mentioned in the first article in this series, defining a custom resource is equivalent to defining an API: it's a contract with the Kubernetes API. And, indeed, you create a CR with &lt;code&gt;operator-sdk&lt;/code&gt; by using the &lt;code&gt;create api&lt;/code&gt; command, where you specify the kind of your CR and its version (&lt;code&gt;ExposedApp&lt;/code&gt; and &lt;code&gt;v1alpha1&lt;/code&gt;, respectively, for this example):&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;operator-sdk create api --version v1alpha1 --kind ExposedApp &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Running this command results in the following project structure:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;. ├── Makefile ├── PROJECT ├── pom.xml └── src └── main ├── java │ └── io │ └── halkyon │ ├── ExposedApp.java │ ├── ExposedAppReconciler.java │ ├── ExposedAppSpec.java │ └── ExposedAppStatus.java └── resources └── application.properties 6 directories, 8 files &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In particular, you can see that you have an &lt;code&gt;ExposedApp&lt;/code&gt; class representing your CR, which matches the code shown above, with its associated &lt;code&gt;ExposedAppSpec&lt;/code&gt; and &lt;code&gt;ExposedAppStatus&lt;/code&gt; classes. More interestingly, a &lt;code&gt;Reconciler&lt;/code&gt; implementation is also generated, parameterized with your &lt;code&gt;ExposeApp&lt;/code&gt; CR class: &lt;code&gt;ExposedAppReconciler&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Notice that if you had run this last command in a different shell than the one where you had previously launched Quarkus dev mode, you would see that the application automatically restarted and that the extension processed your code:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-26) Registered 'io.halkyon.ExposedApp' for reflection INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-26) Registered 'io.halkyon.ExposedAppSpec' for reflection INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-26) Registered 'io.halkyon.ExposedAppStatus' for reflection ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Quarkus extension lets you know that it registered the classes associated with your CR to be accessed via Java reflection. This is required for your Operator to work properly when compiled natively. Without the extension, you would have needed to configure &lt;a href="https://www.graalvm.org"&gt;GraalVM&lt;/a&gt; accordingly.&lt;/p&gt; &lt;p&gt;As previously mentioned, your CR class is annotated with the &lt;code&gt;@Group&lt;/code&gt; and &lt;code&gt;@Version&lt;/code&gt; annotations, utilizing the information you passed to the &lt;code&gt;operator-sdk&lt;/code&gt; tool. In particular, this information is used to automatically define the resource name associated with your CR and register your reconciler with the automatically created &lt;code&gt;Operator&lt;/code&gt; instance:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-14) Processed 'io.halkyon.ExposedAppReconciler' reconciler named 'exposedappreconciler' for 'exposedapps.halkyon.io' resource (version 'halkyon.io/v1alpha1') &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;More interestingly, though, the extension also automatically generates a CRD associated with your custom resource:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.fab.crd.gen.CRDGenerator] (build-14) Generating 'exposedapps.halkyon.io' version 'v1alpha1' with io.halkyon.ExposedApp (spec: io.halkyon.ExposedAppSpec / status io.halkyon.ExposedAppStatus)... INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-14) Generated exposedapps.halkyon.io CRD: INFO [io.qua.ope.dep.OperatorSDKProcessor] (build-14) - v1 -&gt; &lt;path to your application&gt;/target/kubernetes/exposedapps.halkyon.io-v1.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;CRDs are to CRs what Java classes are to Java instances: they describe and validate the structure of associated CRs by specifying the name and type of each of your CR fields. The CRD is how the Kubernetes API learns what to expect from your newly added API. CRDs are, like almost everything Kubernetes-related, also resources, and a little tricky to create manually. Therefore, it's really interesting that the Quarkus extension for JOSDK can automatically generate the CRD from your code. The extension will also keep your CRD in sync with any changes you might make to classes that might affect the CRD (i.e., the tree of classes on which your CR depends)—and does so &lt;em&gt;only&lt;/em&gt; in that particular case, thus avoiding wasted efforts if the CRD doesn't need to be generated. This results in a more fluid experience while live-coding your Operator.&lt;/p&gt; &lt;p&gt;That said, you're still getting an error from the Operator. If you have kept Quarkus dev mode running, you should see a message similar to this:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt; ERROR [io.qua.run.Application] (Quarkus Main Thread) Failed to start application (with profile dev): io.javaoperatorsdk.operator.MissingCRDException: 'exposedapps.halkyon.io' v1 CRD was not found on the cluster, controller 'exposedappreconciler' cannot be registered &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The message is fairly explicit: you probably have not deployed your CRD to your cluster yet. By default, to help avoid somewhat cryptic HTTP 404 errors during development, JOSDK checks if the associated CRD is present on the cluster before starting a controller. This behavior can be configured, and it is indeed recommended that you deactivate this check in production because it requires escalated permissions for the Operator.&lt;/p&gt; &lt;p&gt;At this point, if you were not using the Quarkus extension, you would need to stop what you were working on and drop back to the console to apply the CRD on the cluster:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;kubectl apply -f target/kubernetes/exposedapps.halkyon.io-v1.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wouldn't it be better if you didn't have to stop working on your code each time the CRD is regenerated? Quarkus extension to the rescue! A configuration property named &lt;code&gt;quarkus.operator-sdk.crd.apply&lt;/code&gt; offers just this possibility. To make your life easier, &lt;code&gt;operator-sdk&lt;/code&gt; even added it to your Operator's &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class="properties"&gt;# set to true to automatically apply CRDs to the cluster when they get regenerated quarkus.operator-sdk.crd.apply=false &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you modify the property to set it to &lt;code&gt;true&lt;/code&gt;, you'll see that Quarkus restarts your application, and you can observe the following message:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.qua.dep.dev.RuntimeUpdatesProcessor] (pool-1-thread-1) Restarting quarkus due to changes in application.properties. ... INFO [io.qua.ope.run.OperatorProducer] (Quarkus Main Thread) Applied v1 CRD named 'exposedapps.halkyon.io' from &lt;path to your application&gt;/target/kubernetes/exposedapps.halkyon.io-v1.yml ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that it is likely that this behavior will be made automatic when using the Quarkus dev mode: you won't have to specify this property anymore, the extension will set it by default to make the experience even more seamless.&lt;/p&gt; &lt;p&gt;This time, your Operator starts correctly and tells you that it applied the CRD to your cluster. Thanks to this development mode, you will be able to progressively enrich your model without having to leave your code editor to stop and either generate the CRD again or apply it to the cluster each time you change your Java classes. In particular, remember that the &lt;code&gt;ExposedApp&lt;/code&gt; CR exposes an &lt;code&gt;imageRef&lt;/code&gt; field as part of its &lt;code&gt;spec&lt;/code&gt; to specify the application you want to expose via your Operator.&lt;/p&gt; &lt;p&gt;Add such an &lt;code&gt;imageRef&lt;/code&gt; &lt;code&gt;String&lt;/code&gt; field to your &lt;code&gt;ExposedAppSpec&lt;/code&gt; class. You can see in the logs that the Quarkus extension restarted the Operator, and that the CRD was regenerated (since you changed a class that impacts its content) and re-applied to your cluster.&lt;/p&gt; &lt;h2&gt;Conclusion, and a look ahead&lt;/h2&gt; &lt;p&gt;This article has covered quite a bit of ground, looking at how JOSDK and its Quarkus extension help you stay in the flow while modeling your Operator's domain, and, dare we say, it's made the whole experience more enjoyable. In the next article in this series, you'll add the logic to your reconciler to create the Kubernetes resources required to expose your application.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2" title="Write Kubernetes in Java with the Java Operator SDK, Part 2"&gt;Write Kubernetes in Java with the Java Operator SDK, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2022-03-22T07:00:00Z</dc:date></entry><entry><title>Deploy Infinispan automatically with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/21/deploy-infinispan-automatically-ansible" /><author><name>Romain Pelisse</name></author><id>30c57a95-23dd-4486-9c33-9c487565aeaa</id><updated>2022-03-21T07:00:00Z</updated><published>2022-03-21T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://infinispan.org/"&gt;Infinispan&lt;/a&gt;, an in-memory data store, is popular among &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; programmers as a fast and scalable key-value store that can be deployed in a variety of settings. This article describes how to use &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible&lt;/a&gt; to automate the installation and setup of an Infinispan instance. Along the way, you'll learn about many aspects of Ansible's capabilities.&lt;/p&gt; &lt;p&gt;The most straightforward use for Infinispan is as an in-memory cache, embedded into an application. Infinispan can also be a separate, standalone cache offering transient memory to separate applications that access the cache using different protocols. A third option is to treat Infinispan as a kind of NoSQL database. And these are only the most common use cases; additional options are available.&lt;/p&gt; &lt;p&gt;Infinispan can be resilient, even across data centers or availability zones, thanks to its efficient &lt;a href="https://infinispan.org/docs/stable/titles/xsite/xsite.html"&gt;cross-site replication&lt;/a&gt; and &lt;a href="https://infinispan.org/docs/stable/titles/tuning/tuning.html#performance-partition-handling_deployment-planning"&gt;split-brain mitigation options&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;With so many use cases at one's disposal, it is nice to have a dedicated &lt;a href="https://github.com/ansible-middleware/infinispan"&gt;Ansible collection&lt;/a&gt; to help automate Infinispan deployment and configuration. This article provides a walkthrough of Ansible's usage and shows you how to integrate Infinispan easily into an Ansible Playbook to manage the software like any other piece of the application's infrastructure.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To follow this tutorial, you need a &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; or Fedora system, along with Ansible (preferably, the latest version of the 2.9 branch).&lt;/p&gt; &lt;h2&gt;Install the collection&lt;/h2&gt; &lt;p&gt;The code for this tutorial is stored in a package—a &lt;em&gt;collection,&lt;/em&gt; in Ansible terminology—provided by a repository called the &lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt;. This collection is named &lt;code&gt;middleware_automation.infinispan&lt;/code&gt;. When you install it, Ansible Galaxy also automatically pulls in a dependency named &lt;code&gt;middleware_automation.redhat_csp&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.infinispan Process install dependency map Starting collection install process Installing 'middleware_automation.infinispan:0.1.7' to '/home/rpelisse/.ansible/collections/ansible_collections/middleware_automation/infinispan' Installing 'middleware_automation.redhat_csp_download:1.2.1' to '/home/rpelisse/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Depending on the configuration of the machine used as the Ansible controller, you may need to add some Python dependencies so that Ansible has the required libraries to make use of the collection. This is easily achieved by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ pip3 install lxml jmespath Collecting lxml Downloading lxml-4.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB) |████████████████████████████████| 6.9 MB 1.9 MB/s Collecting jmespath Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Installing collected packages: lxml, jmespath Successfully installed jmespath-0.10.0 lxml-4.7.1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now that the collection is installed, you can insert it into a playbook, which you should name &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;--- - name: Playbook for infinispan Hosts hosts: infinispan become: yes collections: - middleware_automation.infinispan tasks: &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before going further, let's check that installation of the collection was successful by running this empty playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# ansible-playbook -i inventory/localhost playbook.yml PLAY [Playbook for infinispan Hosts] *************************************************************************************************** TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install Infinispan with Ansible&lt;/h2&gt; &lt;p&gt;Thanks to the collection you just installed, the task in this section is very easy. However, before you look at how to implement the task inside your playbook, you need to understand what I mean here by "installing Infinispan." Indeed, the task encompasses quite a few actions that Ansible performs on the target system:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Creating appropriate user and group accounts&lt;/li&gt; &lt;li&gt;Downloading the archive from the Infinispan website&lt;/li&gt; &lt;li&gt;Unarchiving the content while ensuring that all the files are associated with the appropriate user and groups&lt;/li&gt; &lt;li&gt;Ensuring that the required version of the Java Virtual Machine (JVM) is installed&lt;/li&gt; &lt;li&gt;Integrating the software into the host service management system (in this case, &lt;a href="https://systemd.io"&gt;systemd&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;And much more&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To carry out all the tasks on this list, all you need to do is add to your playbook to provide a secure password for administrative access to the Infinispan server and specify the appropriate role from the collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;--- - name: Playbook for infinispan Hosts hosts: infinispan become: yes collections: - middleware_automation.infinispan vars: jdg_supervisor_password: "thisshouldbeaverysecurepassword" tasks: - name: "infinispan" include_role: name: infinispan vars: supervisor_password: "{{ jdg_supervisor_password }}" infinispan_users: []&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What's going on here? First, the &lt;code&gt;middleware_automation.infinispan&lt;/code&gt; collection is in the list of collections used by this playbook. Then you have a variable to hold the supervisor password of the Infinispan instance. Note that because this variable is a password, it should really be secured using &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/vault.html"&gt;Ansible Vault&lt;/a&gt;. However, that task is beyond the scope of this article.&lt;/p&gt; &lt;p&gt;The last and most important aspect of this playbook is the use of the &lt;code&gt;infinispan&lt;/code&gt; collection. You've placed this name in the list of collections used by the playbook, and then used its content in a role called &lt;code&gt;infinispan&lt;/code&gt;. This role is provided by the collection that was previously installed and requires you to provide two pieces of information as variables: the supervisor password and the list of Infinispan users.&lt;/p&gt; &lt;p&gt;At this stage, you've kept things simple and specified no extra users for Infinispan. Instead, you've retained the administrator account called supervisor, as explained in the &lt;a href="https://infinispan.org/docs/stable/titles/security/security.html"&gt;Infinispan security documentation&lt;/a&gt;. This account must be secured with a password, so you defined the appropriate value in the &lt;code&gt;tasks&lt;/code&gt; property, which is executed before the roles.&lt;/p&gt; &lt;p&gt;Now run this playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-playbook -i inventory/localhost playbook.yml PLAY [Playbook for infinispan Hosts] ********************************************************************************************************************************************************* TASK [Gathering Facts] *********************************************************************************************************************************************************************** ok: [localhost] TASK [infinispan] **************************************************************************************************************************************************************************** TASK [middleware_automation.infinispan.infinispan : Validate parameters] ********************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Include prerequisite tasks] ************************************************************************************************************** included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/prereqs.yml for localhost TASK [middleware_automation.infinispan.infinispan : Validate credentials] ******************************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Set required packages facts] ************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Ensures required packages are installed] ************************************************************************************************* included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/fastpackages/install.yml for localhost TASK [middleware_automation.infinispan.infinispan : Set required packages facts] ************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check packages to be installed] ********************************************************************************************************** included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/fastpackages/check.yml for localhost =&gt; (item=unzip) included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/fastpackages/check.yml for localhost =&gt; (item=procps-ng) included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/fastpackages/check.yml for localhost =&gt; (item=initscripts) included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/fastpackages/check.yml for localhost =&gt; (item=java-1.8.0-openjdk-devel) TASK [middleware_automation.infinispan.infinispan : Check if package unzip is already installed] ********************************************************************************************* fatal: [localhost]: FAILED! =&gt; {"changed": true, "cmd": ["rpm", "-q", "unzip"], "delta": "0:00:00.046008", "end": "2022-02-10 14:42:22.708736", "msg": "non-zero return code", "rc": 1, "start": "2022-02-10 14:42:22.662728", "stderr": "", "stderr_lines": [], "stdout": "package unzip is not installed", "stdout_lines": ["package unzip is not installed"]} TASK [middleware_automation.infinispan.infinispan : If package unzip is missing, add it to the yum install list.] **************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check if package procps-ng is already installed] ***************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check if package initscripts is already installed] *************************************************************************************** fatal: [localhost]: FAILED! =&gt; {"changed": true, "cmd": ["rpm", "-q", "initscripts"], "delta": "0:00:00.010029", "end": "2022-02-10 14:42:23.259519", "msg": "non-zero return code", "rc": 1, "start": "2022-02-10 14:42:23.249490", "stderr": "", "stderr_lines": [], "stdout": "package initscripts is not installed", "stdout_lines": ["package initscripts is not installed"]} TASK [middleware_automation.infinispan.infinispan : If package initscripts is missing, add it to the yum install list.] ********************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check if package java-1.8.0-openjdk-devel is already installed] ************************************************************************** fatal: [localhost]: FAILED! =&gt; {"changed": true, "cmd": ["rpm", "-q", "java-1.8.0-openjdk-devel"], "delta": "0:00:00.010355", "end": "2022-02-10 14:42:23.536096", "msg": "non-zero return code", "rc": 1, "start": "2022-02-10 14:42:23.525741", "stderr": "", "stderr_lines": [], "stdout": "package java-1.8.0-openjdk-devel is not installed", "stdout_lines": ["package java-1.8.0-openjdk-devel is not installed"]} TASK [middleware_automation.infinispan.infinispan : If package java-1.8.0-openjdk-devel is missing, add it to the yum install list.] ********************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Install packages: ['unzip', 'initscripts', 'java-1.8.0-openjdk-devel']] ****************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Create group jdg] ************************************************************************************************************************ changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Create user jdg] ************************************************************************************************************************* changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Create download directory] *************************************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Include install tasks] ******************************************************************************************************************* included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/install.yml for localhost TASK [middleware_automation.infinispan.infinispan : Set download archive path] *************************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check download archive path] ************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Check local download archive path] ******************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Download infinispan archive] ************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Performing download from RHN] ************************************************************************************************************ skipping: [localhost] TASK [middleware_automation.infinispan.infinispan : Download DataGrid archive from alternate location] *************************************************************************************** skipping: [localhost] TASK [middleware_automation.infinispan.infinispan : Check downloaded archive] **************************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Copy archive to target nodes] ************************************************************************************************************ changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Check target directory: /opt/infinispan/infinispan-server-12.1.7.Final/] ***************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Extract Infinispan archive on target] **************************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Inform decompression was not executed] *************************************************************************************************** skipping: [localhost] TASK [middleware_automation.infinispan.infinispan : Reown installation directory to jdg] ***************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Installation directory] ****************************************************************************************************************** ok: [localhost] =&gt; { "msg": "Infinispan installed at path /opt/infinispan/infinispan-server-12.1.7.Final/" } TASK [middleware_automation.infinispan.infinispan : Include systemd tasks] ******************************************************************************************************************* included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/systemd.yml for localhost TASK [middleware_automation.infinispan.infinispan : Configure systemd unit file for infinispan service] ************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Perform daemon-reload to ensure the changes on infinispan service are picked up] ********************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Parse declarative cache config to cache xml] ********************************************************************************************* TASK [middleware_automation.infinispan.infinispan : Ensures infinispan configuration is deployed: jdg.xml] *********************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Ensures infinispan log4j2 configuration is deployed] ************************************************************************************* changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Download database driver jar to target] ************************************************************************************************** skipping: [localhost] TASK [middleware_automation.infinispan.infinispan : Include users tasks] ********************************************************************************************************************* included: /root/.ansible/collections/ansible_collections/middleware_automation/infinispan/roles/infinispan/tasks/jdg_user.yml for localhost TASK [middleware_automation.infinispan.infinispan : Validate parameters] ********************************************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Ensures users.properties exists.] ******************************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Ensures groups.properties exists] ******************************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Ensures infinispan service is running and enabled] *************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Link default logs directory] ************************************************************************************************************* changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Flush handlers] ************************************************************************************************************************** RUNNING HANDLER [middleware_automation.infinispan.infinispan : restart infinispan] *********************************************************************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Wait for used port to be open] *********************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan : Include tasks to validate keycloak remote caches] **************************************************************************************** skipping: [localhost] PLAY RECAP *********************************************************************************************************************************************************************************** localhost : ok=43 changed=14 unreachable=0 failed=0 skipped=6 rescued=3 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Quite a lot has happened here. More than 40 tasks have been performed that take care of the configuration tasks mentioned in the earlier list (creating users and a group, downloading the software, and installing the required JVM, among others). Once the playbook finishes its execution, you can confirm that Infinispan is now running as a service by verifying its status:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status infinispan ● infinispan.service - infinispan service Loaded: loaded (/etc/systemd/system/infinispan.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2022-02-10 14:43:04 UTC; 8min ago Process: 924 ExecStart=/bin/sh -c /opt/infinispan/infinispan-server-12.1.7.Final//bin/server.sh -c jdg.xml -b localhost &amp; (code=exited, status=0/SUCCESS) Main PID: 1008 (java) Tasks: 52 (limit: 1638) Memory: 329.5M CGroup: /system.slice/infinispan.service ├─ 925 /bin/sh /opt/infinispan/infinispan-server-12.1.7.Final//bin/server.sh -c jdg.xml -b localhost └─1008 java -server -verbose:gc -Xloggc:/opt/infinispan/infinispan-server-12.1.7.Final/server/log/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseGCLogFileRotation -XX:&gt; Feb 10 14:43:09 310305941c20 sh[924]: 2022-02-10 14:43:09,526 INFO (main) [org.infinispan.CLUSTER] ISPN000079: Channel datagrid local address is localhost, physical addresses are [10.0.2.1&gt; Feb 10 14:43:09 310305941c20 sh[924]: 2022-02-10 14:43:09,551 INFO (main) [org.infinispan.CONTAINER] ISPN000390: Persisted state, version=12.1.7.Final timestamp=2022-02-10T14:43:09.549Z Feb 10 14:43:09 310305941c20 sh[924]: 2022-02-10 14:43:09,861 INFO (main) [org.jboss.threads] JBoss Threads version 2.3.3.Final Feb 10 14:43:09 310305941c20 sh[924]: 2022-02-10 14:43:09,944 INFO (main) [org.infinispan.CONTAINER] ISPN000104: Using EmbeddedTransactionManager Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,343 INFO (main) [org.infinispan.server.core.RequestTracer] OpenTracing integration is disabled Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,407 INFO (ForkJoinPool.commonPool-worker-1) [org.infinispan.SERVER] ISPN080018: Started connector HotRod (internal) Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,507 INFO (main) [org.infinispan.SERVER] ISPN080018: Started connector REST (internal) Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,808 INFO (main) [org.infinispan.SERVER] ISPN080004: Connector SINGLE_PORT (default) listening on 0.0.0.0:11222 Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,809 INFO (main) [org.infinispan.SERVER] ISPN080034: Server 'localhost' listening on http://0.0.0.0:11222 Feb 10 14:43:10 310305941c20 sh[924]: 2022-02-10 14:43:10,843 INFO (main) [org.infinispan.SERVER] ISPN080001: Infinispan Server 12.1.7.Final started in 5178ms &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the execution has verified that the service is not only running, but is also available:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;… RUNNING HANDLER [middleware_automation.infinispan.infinispan : restart infinispan] ***************************************************** changed: [localhost] TASK [middleware_automation.infinispan.infinispan : Wait for used port to be open] ***************************************************** ok: [localhost] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, for the sake of being thorough, you can double-check that the Infinispan port is indeed accessible:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -I http://localhost:11222 HTTP/1.1 405 Method Not Allowed content-length: 11 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To summarize, at the end of this playbook execution, you have a service running through &lt;code&gt;systemd&lt;/code&gt;, managing an instance of Infinispan. However, this instance is bare; there are no caches configured for an application to use. You'll add one in the next section.&lt;/p&gt; &lt;h2&gt;Deploy caches in Infinispan using Ansible&lt;/h2&gt; &lt;p&gt;To deploy a cache for Infinispan, you need to modify the main configuration file, which is difficult to achieve using templates because it is written in XML. However, here again, the &lt;code&gt;infinispan&lt;/code&gt; collection provides a handy role that greatly simplifies the task. Add the following to &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;… - name: "infinispan cache" include_role: name: infinispan_cache vars: deployer_user: "supervisor" deployer_password: "{{ jdg_supervisor_password }}" cache_config: name: myCache template: replicated&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can find more information about how to configure caches (dynamically, as shown here, or statically) in the &lt;a href="https://github.com/ansible-middleware/infinispan/blob/main/roles/infinispan/README.md#deploying-custom-cache-configurations"&gt;Ansible collection for Infinispan documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The configuration is once again pretty self-explanatory. You have provided the &lt;code&gt;infinispan_cache&lt;/code&gt; variable giving the user connection information to the role. Because there is no user other than &lt;code&gt;supervisor&lt;/code&gt; in your Infinispan installation, you've simply used that user as the &lt;code&gt;deployer_user&lt;/code&gt; variable, and provided the password using the &lt;code&gt;jdg_supervisor_password&lt;/code&gt; variable you defined in the previous section:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;… TASK [middleware_automation.infinispan.infinispan_cache : Validate parameters - rest api url] ************************************************************************************************ ok: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Validate parameters - cache config] ************************************************************************************************ ok: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Parse cache xml] ******************************************************************************************************************* skipping: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Set cache name from xml] *********************************************************************************************************** skipping: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Set cache name from config] ******************************************************************************************************** ok: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Assemble yaml template into xmlstring] ********************************************************************************************* ok: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Check cache myCache state] ********************************************************************************************************* [WARNING]: Module did not set no_log for password ok: [localhost] TASK [middleware_automation.infinispan.infinispan_cache : Create cache myCache] ************************************************************************************************************** ok: [localhost] … &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To be cautious here, you should double-check that the cache was indeed created by adding the following instructions to your playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;… - name: "Ensures cache was created" ansible.builtin.uri: url: http://localhost:11222/rest/v2/caches/myCache method: GET url_username: "supervisor" url_password: "{{ jdg_supervisor_password }}" status_code: - 200 …&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Add content to the cache&lt;/h2&gt; &lt;p&gt;Infinispan is now running as a service under the control of &lt;code&gt;systemd&lt;/code&gt;, and you have set up a cache called &lt;code&gt;myCache&lt;/code&gt;, but there is still no data in it. Although the application using this Infinispan service would normally add data, you can test Infinispan by adding some content to the cache manually via the REST API. To do so, add the following to &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;… - name: "Check service operational: add entry to test cache via rest" ansible.builtin.uri: url: http://localhost:11222/rest/v2/caches/myCache/firstEntry method: POST url_username: "supervisor" url_password: "{{ jdg_supervisor_password }}" status_code: - 204 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Execute your playbook one final time with this extra task:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;… TASK [Check service operational: add entry to test cache via rest] *************************************************************************************************************************** ok: [localhost] … &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Ansible and the Ansible collection for Infinispan have fully automated the deployment of Infinispan instances. Ansible performed all the work: downloading software, preparing the environment (user, group, firewall), deploying the binary files and the configuration, setting up the service in &lt;code&gt;systemd&lt;/code&gt;, etc. Perhaps this article has encouraged you to try Infinispan in your own projects.&lt;/p&gt; &lt;p&gt;You've also gotten some insights into the versatility of Ansible, which can automate many aspects of program builds and deployments. If you're interested in learning more about Ansible and middleware, you might want to read an article I wrote about &lt;a href="https://www.opensourcerers.org/2022/03/14/ansible-middleware-automation-of-jboss-web-server-jws/"&gt;automating Red Hat JBoss Web Server&lt;/a&gt;. And look for a future article here at Red Hat Developer where you'll learn how Ansible can tackle more complex Infinispan configurations.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/21/deploy-infinispan-automatically-ansible" title="Deploy Infinispan automatically with Ansible"&gt;Deploy Infinispan automatically with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2022-03-21T07:00:00Z</dc:date></entry><entry><title>Node.js community update</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/18/nodejs-community-update" /><author><name>Bethany Griggs, Michael Dawson</name></author><id>cb8abf96-8d45-4b2e-90e3-94832c4c2a6c</id><updated>2022-03-18T07:00:00Z</updated><published>2022-03-18T07:00:00Z</published><summary type="html">&lt;p&gt;The Red Hat &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; team dedicates a lot of time to working in the Node.js community and contributing upstream. This is a quick update on some of the interesting things going on in the project as we start 2022.&lt;/p&gt; &lt;h2&gt;Releases&lt;/h2&gt; &lt;p&gt;The Node.js release schedule aims to be predictable so that consumers and users can plan their migrations. As per the schedule (Figure 1):&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Node.js 12 will go end-of-life at the end of April 2022.&lt;/li&gt; &lt;li&gt;Node.js 14 and 16 are both in their long-term support phase and will be supported until April 2023 and April 2024 respectively.&lt;/li&gt; &lt;li&gt;Node.js 17 is our current release line and will be supported until June 2022.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2022Q1-release-timeline.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/2022Q1-release-timeline.png?itok=gjWA3vIJ" width="960" height="500" alt="The Node.js project has released a timeline for a couple upcoming releases (source: https://github.com/nodejs/release)." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Node.js project has released a timeline for a couple upcoming releases (source: https://github.com/nodejs/release).&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Also keeping with tradition, April will bring a new major version of Node.js: version 18. Preparations are already underway in the project to get things ready for the new release. The &lt;a href="https://github.com/nodejs/build"&gt;Node.js Build Working Group&lt;/a&gt; is planning for any required operating system and compiler upgrades. Early preview builds will likely be available in March.&lt;/p&gt; &lt;h2&gt;New features&lt;/h2&gt; &lt;p&gt;As always, contributors are working hard to bring new features to Node.js, listed as &lt;a href="https://github.com/nodejs/node/pulls?q=is%3Apr+label%3Asemver-minor+%7C%7C+semver-major+is%3Aclosed"&gt;pull requests on GitHub&lt;/a&gt;. Here we detail a few recent additions.&lt;/p&gt; &lt;h3&gt;HTTP fetch()&lt;/h3&gt; &lt;p&gt;&lt;code&gt;fetch()&lt;/code&gt; is going to happen! The experimental implementation of this long-awaited method has landed and shipped in Node.js &lt;a href="https://nodejs.org/en/blog/release/v17.5.0/"&gt;17.5.0&lt;/a&gt;. Global &lt;code&gt;fetch()&lt;/code&gt; will be available initially only when you use the &lt;code&gt;--experimental-fetch&lt;/code&gt; command-line option. The implementation is based upon &lt;a href="http://nodejs.github.io/undici"&gt;undici&lt;/a&gt;, an HTTP/1.1 client written for Node.js by contributors to the project. A sample use is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;import {fetch} from 'undici'; async function fetchJson() { const res = await fetch('https://api.github.com/zen'); console.log(res); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The shipping of this feature was accelerated in part by discussions in the most recent Node.js Next 10 mini-summit (January 2022), where the project defined and agreed on priorities for HTTP in Node.js. You can &lt;a href="https://youtu.be/8t6HZfBZck0?t=540"&gt;watch the discussion&lt;/a&gt; on YouTube or &lt;a href="https://github.com/nodejs/node/blob/master/doc/contributing/maintaining-http.md"&gt;read the summary on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;ECMAScript modules&lt;/h3&gt; &lt;p&gt;In recent months, the project has continued to make progress implementing ECMAScript modules. Key milestones include &lt;a href="https://github.com/nodejs/node/pull/41736"&gt;unflagging JSON modules&lt;/a&gt;. Although JSON modules remain experimental, they're now enabled by default as of Node.js 17.5.0.&lt;/p&gt; &lt;p&gt;Support for JSON import assertions has also been added, demonstrated by the following syntax:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;import example from './example.json' assert { type: 'json' };&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Other new features&lt;/h3&gt; &lt;p&gt;Some other developments that show promise include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;An experimental subset of the &lt;a href="https://www.w3.org/community/wicg/"&gt;Web Platform Incubator Community Group&lt;/a&gt; (WICG) scheduling APIs have been added, including the &lt;code&gt;scheduler.wait()&lt;/code&gt; and &lt;code&gt;scheduler.yield()&lt;/code&gt; methods. See the RFC &lt;a href="https://github.com/nodejs/node/pull/40909"&gt;timers: add experimental scheduler api #40909&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;An &lt;a href="https://datatracker.ietf.org/doc/html/rfc2818"&gt;RFC 2818&lt;/a&gt; (HTTP over TLS) compatible &lt;code&gt;checkHost()&lt;/code&gt; method has been added to the x509 certificate support. See the RFC &lt;a href="https://github.com/nodejs/node/pull/41569"&gt;crypto: support RFC 2818 compatible checkHost #41569&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;An experimental &lt;code&gt;node_api_symbol_for()&lt;/code&gt; method has been added to Node.js API to simplify the use of symbols in native addons. See the Pull Request &lt;a href="https://github.com/nodejs/node/pull/41329"&gt;node-api: add node_api_symbol_for() #41329&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Hot topics in the Node.js community&lt;/h2&gt; &lt;p&gt;The Node.js project has a wide and varied collaborator base. This is one of its strengths and means that we have in-depth discussions before some decisions are made. Some of the interesting topics currently being discussed are listed in the following sections.&lt;/p&gt; &lt;h3&gt;Primordials in Node.js core&lt;/h3&gt; &lt;p&gt;Primordials are used in Node.js core in an effort to prevent tampering with the built-in Node.js modules, including from prototype pollution-type attacks.&lt;/p&gt; &lt;p&gt;Use of primordials in Node.js core has been an ongoing discussion within the project and the &lt;a href="https://github.com/nodejs/TSC"&gt;Node.js Technical Steering Committee&lt;/a&gt; (TSC). The discussion concerns the balance between how tamper-proof the core APIs should be, the value of being tamper-proof, and the costs in terms of code maintenance, runtime performance overhead, and the effort required to avoid regressions. This discussion continues, while the TSC has issued its &lt;a href="https://github.com/nodejs/TSC/pull/1158"&gt;first vote&lt;/a&gt; on what should be done in the error path.&lt;/p&gt; &lt;h3&gt;Updating the toolchain used to build Node.js&lt;/h3&gt; &lt;p&gt;Discussion about what tools we should use to build Node.js is heating up again. The current toolchain introduces some challenges, and the project would like to make a change. Some of the options appear in the following documents:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/nodejs/build-toolchain-next/pull/9"&gt;analysis: add cmake toolchain analysis #9&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/nodejs/build-toolchain-next/pull/8"&gt;analysis: add Bazel toolchain analysis #8&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/nodejs/build-toolchain-next/pull/7"&gt;analysis: add GN toolchain analysis #7&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;There is a &lt;a href="https://github.com/nodejs/build-toolchain-next/issues/11"&gt;call for interested people to help move this migration forward&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Next 10: The future of Node.js&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://github.com/nodejs/next-10"&gt;Node.js Next 10&lt;/a&gt; group continues to work through the process of surfacing and documenting key &lt;a href="https://github.com/nodejs/node/blob/master/doc/contributing/technical-priorities.md"&gt;technical priorities&lt;/a&gt; for the project. The team led two mini-summits recently. One took place in &lt;a href="https://github.com/nodejs/next-10/blob/main/meetings/summit-nov-2021.md"&gt;November 2021&lt;/a&gt; to cover suitable types for end-users and single executable applications. Another took place in &lt;a href="https://github.com/nodejs/next-10/blob/main/meetings/summit-jan-2022.md"&gt;January 2022&lt;/a&gt; to topics covered modern HTTP implementations and documentation.&lt;/p&gt; &lt;p&gt;We've had good engagement in these two summits, leading to some concrete outcomes. As an example, thanks to the HTTP discussions, as mentioned earlier, &lt;code&gt;fetch()&lt;/code&gt; is now an experimental API. We also agreed to explore updating the style guide to allow us to generate better JSON documentation that can be used to generate type information more easily.&lt;/p&gt; &lt;p&gt;If you want to help Node.js be successful going forward, the Next 10 group is a great way to do so. You can participate in our &lt;a href="https://github.com/nodejs/next-10"&gt;GitHub repository&lt;/a&gt; and attend the Next 10 meetings and mini-summit, which can be found in the &lt;a href="https://nodejs.org/calendar"&gt;Node.js Project Meeting Calendar&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Catch up with the Node.js Next 10 mini-summit recordings:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=TY5uirDq1SI"&gt;November 2022 mini-summit recording (Types and single binary executables)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=8t6HZfBZck0&amp;t=540s"&gt;January 2022 mini-summit recording (Modern HTTP and documentation)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Recent and upcoming events&lt;/h2&gt; &lt;p&gt;While it's still a challenge to travel to conferences, we are hopeful that things will be better when the time comes for &lt;a href="https://events.linuxfoundation.org/openjs-world/"&gt;OpenJS World, Austin&lt;/a&gt; in June 2022. Our team has made several submissions to the conference and hopes to be able to see you at both the event and the Node.js Collaborator Summit in the following few days for a long-delayed reunion. In the meantime, we've been presenting virtually at &lt;a href="https://devconf.cz/"&gt;DevConf.cz&lt;/a&gt;, and we'll also be speaking on &lt;a&gt;Navigating the npm Ecosystem in the Enterprise&lt;/a&gt; at the upcoming &lt;a href="https://cityjsconf.org/speaker/6SKxTCSe5pwg2irsOy5Ong"&gt;CityJS Conference, London&lt;/a&gt; on March 25, 2022.&lt;/p&gt; &lt;h2&gt;Stay up to date on Node.js&lt;/h2&gt; &lt;p&gt;We hope you enjoyed our short update on what's going on in the Node.js community. If you want to read more, check out:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js on Red Hat Developer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developer.ibm.com/languages/node-js/"&gt;Node.js on IBM Developer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://nodejs.org/en/blog/"&gt;The Node.js project blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/18/nodejs-community-update" title="Node.js community update"&gt;Node.js community update&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bethany Griggs, Michael Dawson</dc:creator><dc:date>2022-03-18T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #18 - March</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-18/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-18/</id><updated>2022-03-17T00:00:00Z</updated><published>2022-03-17T00:00:00Z</published><summary type="html">The March newsletter has been sent. Learn about what’s new with Java 17 and containers, read about LogicDrop and Vaadin’s path to using Quarkus, and get a look at Continuous testing with Quarkus. Additionally, you’ll get more insights into how Keycloak is ow fully cloud-native with Quarkus and learn how...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2022-03-17T00:00:00Z</dc:date></entry><entry><title>Choose the best camel for your integration ride, Part 3</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/16/choose-best-camel-your-integration-ride-part-3" /><author><name>Bruno Meseguer</name></author><id>977c5163-86f5-42a5-903e-4ff5178d206a</id><updated>2022-03-16T07:00:00Z</updated><published>2022-03-16T07:00:00Z</published><summary type="html">&lt;p&gt;The previous installments in this series &lt;a href="https://developers.redhat.com/articles/2022/01/26/choose-best-camel-your-integration-ride-part-1"&gt;explored the motivations behind different community-supported runtimes&lt;/a&gt; for &lt;a href="https://camel.apache.org"&gt;Apache Camel&lt;/a&gt; and &lt;a href="https://developers.redhat.com/articles/2022/01/26/choosing-best-camel-your-integration-ride-part-2"&gt;enumerated in detail the benefits of each runtime&lt;/a&gt;. This final installment gives you a simplified and opinionated decision flow containing basic questions to help you identify which Camel runtime will work best for you.&lt;/p&gt; &lt;p&gt;The obvious first question is what happens if your organization mandates all aspects of the development process, including the technologies and runtimes to use. If that's the case, the guidance in this series of articles would appear of no use. You can, however, suggest what in your view could be a positive change to their current choices.&lt;/p&gt; &lt;h2&gt;Choose the right Camel runtime for your environment&lt;/h2&gt; &lt;p&gt;The decision tree in Figure 1 tries to recommend the best Camel runtime for you, depending on your target environment.&lt;/p&gt; &lt;p&gt;The major division in Figure 1 is between &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; and non-containerized environments. The first part of this series described the evolution of integration applications and how the emergence of containers accelerated the shift to &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice architectures&lt;/a&gt;. This transition does not necessarily mean that all integration projects must become microservices running in containers. The trend just added new miles to our road—that is, new places to survey during your decision-making.&lt;/p&gt; &lt;p&gt;It's up to you whether or not to embrace &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, depending on your requirements and budget. If you don't, remember Kubernetes as a possible target environment. I's also wise to keep Kubernetes in mind in case plans come up in your organization to transition to containers and microservices.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/decision-diagram.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/decision-diagram.jpg?itok=cQNFvd-t" width="1440" height="829" alt="Containers, complexity, and developer control are the main factors determining the best runtime for Apache Camel." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Containers, complexity, and developer control are the main factors determining the best runtime for Apache Camel. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Camel for standalone applications&lt;/h2&gt; &lt;p&gt;For many developers, running integrations in a Kubernetes environment is not an option or is simply overkill. In that case, choose a standalone Camel runtime.&lt;/p&gt; &lt;p&gt;If you'll be running standalone, and you're not planning to implement many services, consider adopting Camel Quarkus for its simplicity, performance, super low memory footprint, and compatibility with Camel 3.&lt;/p&gt; &lt;p&gt;Another good reason to prefer Camel Quarkus over other standalone runtimes is that it represents the future of Camel, where the community invests most of its effort. You should also consider that while today you may be running just in standalone mode, Camel Quarkus leaves an open door to comfortably transition to container environments when the opportunity arises.&lt;/p&gt; &lt;p&gt;However, if you foresee the need to implement extra services, consider bundling your application in a fully featured OSGi container such as Karaf. Try to ensure that you keep your Camel projects small and independent, avoiding monolithic structures that are challenging to evolve; OSGI's architecture should help you with that.&lt;/p&gt; &lt;h2&gt;Camel for Kubernetes&lt;/h2&gt; &lt;p&gt;Let's look at your options If your target environment is Kubernetes. Because containers favor distributed architectures and encourage splitting big application servers into independent microservices, deploying OSGi engines in containers doesn't make much sense. Running one service per container is the best practice.&lt;/p&gt; &lt;p&gt;To fit the slim image sizes desired for containers and Kubernetes, many developers feel tempted to switch from Java to other languages that provide a lighter and smaller runtime. The downside is that most of these languages lack the maturity of Java, and more importantly, the rich connectivity and functionality specifically designed to solve &lt;a href="https://camel.apache.org/components/next/eips/enterprise-integration-patterns.html"&gt;enterprise integration patterns&lt;/a&gt; that Camel brings.&lt;/p&gt; &lt;p&gt;Up until now, the most popular runtime to deploy Camel in Kubernetes was Spring Boot, which seemed to be a lightweight alternative to existing Java frameworks when it was introduced but now appears relatively heavyweight. Now Quarkus is on the scene to provide supersonic, subatomic Java. Quarkus promises a long life for Camel in the container space, to the delight of many developers already using Java and Camel.&lt;/p&gt; &lt;p&gt;Camel Quarkus provides very economical memory usage and top performance. Thus, Camel Quarkus meets the requirements of serverless platforms, where applications scale to zero when idle and react to incoming traffic by waking up and responding in just a few milliseconds. This behavior, of course, has tremendous benefits in the use of platform resources.&lt;/p&gt; &lt;p&gt;For the reasons cited here, the diagram includes Camel Quarkus and Camel K (Quarkus based) only. We have discarded other non-Quarkus Java runtimes for missing out on those characteristics.  In conclusion, it is a matter of choosing between the two. &lt;/p&gt; &lt;h2&gt;Trading off simplicity and control&lt;/h2&gt; &lt;p&gt;Suppose there are considerable data transformations to be defined and nontrivial processing logic in your service: for instance, interacting with multiple endpoints. In that case, you should retain maximum control of Camel's framework. That's what Camel Quarkus provides and what the traditional Camel developer is used to, except with the much-improved performance that Quarkus achieves. As an analogy, imagine you have complete design control over the electronics that power a multicolor LED bulb (Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bulb-circuit_0.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/bulb-circuit_0.gif" width="753" height="398" alt="Quarkus can be imagined as giving you full control over the electronics of a multicolor LED bulb." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Quarkus can be imagined as giving you full control over the electronics of a multicolor LED bulb.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;However, with today's explosion of data flows, very often, use cases are relatively simple. Developers generally seek to fetch data from data sources or feed data to target platforms, with relatively low data handling or none at all.&lt;/p&gt; &lt;p&gt;That's Camel K's sweet spot. It delivers super connectivity with Camel's rich palette of connectors and can still include a reasonable degree of data manipulation if desired; however, as a basic principle, you should try to keep service options for each application to a minimum. Back to our bulb example, imagine you can turn the light on and off with a knob that controls its intensity (Figure 3).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bulb-dimmer_0.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/bulb-dimmer_0.gif" width="600" height="364" alt="Camel K is comparable to a dimmer switch that controls light intensity." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Camel K is comparable to a dimmer switch that controls light intensity.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Of course, the animation in Figure 3 just illustrates the concept. Still, it might help you visualize how Camel K hides much of the complexity from you, in contrast with the fully designed electronics of the multicolor bulb where full attention to all details is necessary.&lt;/p&gt; &lt;p&gt;Camel K's &lt;a href="https://camel.apache.org/manual/dsl.html"&gt;Domain Specific Language (DSL)&lt;/a&gt; is an excellent choice for simple use cases. The DSL offers some degree of control (the knob controlling light intensity), simplifying the developer's work and letting the Operator manage the service's lifecycle.&lt;/p&gt; &lt;p&gt;To better grasp what a simple use case could look like and the power that Camel K can deliver, check &lt;a href="https://developers.redhat.com/articles/2021/11/24/normalize-web-services-camel-k-and-atlasmap-part-1"&gt;this two-part series&lt;/a&gt; detailing a complete API integration. Following best practices, the example in that series exposes an OpenAPI service, applies data transformation, connects to an endpoint, and processes its response.&lt;/p&gt; &lt;h3&gt;Kamelet bindings&lt;/h3&gt; &lt;p&gt;The last decision in our diagram separates Camel K's traditional developer's style from the "no-code" style of using Kamelet bindings, as introduced in &lt;a href="https://developers.redhat.com/articles/2022/01/26/choosing-best-camel-your-integration-ride-part-2#camel_spring_boot"&gt;Part 2 of this series&lt;/a&gt;. The Kamelet binding construct is meant for an even higher level of abstraction, allowing for configuration-only use cases. The Kamelet binding uses pre-built, out-of-the-box Kamelets. Think of these as processing flows you enable and disable, but to which you don't apply much further control. Again, back to our bulb example, imagine we're turning the light on and off using a switch.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bulb-switch_1.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/bulb-switch_1.gif" width="600" height="365" alt="Kamelets are comparable to a switch that turns the light on and off." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Kamelets are comparable to a switch that turns the light on and off.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you push a Kamelet binding, it's as if you press the switch to turn the light on. The Operator reads its definition and deploys an integration process that executes it. If you later delete the Kamelet binding resource, the Operator stops it and tears it down, as if switching the light off.&lt;/p&gt; &lt;p&gt;For more formal examples, look at &lt;a href="https://developers.redhat.com/articles/2021/09/14/improve-cross-team-collaboration-camel-k"&gt;this article&lt;/a&gt; that explains the use of both the DSL and Kamelet bindings. If you look at steps 1 and 3 of the article, in particular, you'll see how the configuration-only blocks enable integration stages of the end-to-end data flow.&lt;/p&gt; &lt;p&gt;Although Kamelet bindings can seem very restrictive, if you'd like a bit more control, you can define new Kamelets in your catalog to satisfy your requirements. In our metaphor, you can unscrew the light switch and tune it to your liking.&lt;/p&gt; &lt;h2&gt;Final words&lt;/h2&gt; &lt;p&gt;Apache Camel is known as the Swiss knife of integration, and you probably grasped by now that Camel goes even further than that. It offers a wide variety of different runtimes that help propagate a solid foundation for integrations.&lt;/p&gt; &lt;p&gt;It's helpful to understand Camel's timeline and how it has evolved, continuously looking at new horizons. You can then find out which Camel runtime fits better in your world and take advantage.&lt;/p&gt; &lt;p&gt;You can undoubtedly integrate systems with arbitrary languages and frameworks, but that's like random shooting from recoil. Apache Camel is like harnessing standard light energy to provide you with a sharp laser-pointing focus.&lt;/p&gt; &lt;h2 id="learn_more_about_camel_k-h2"&gt;Learn more about Camel Quarkus and Camel K&lt;/h2&gt; &lt;p&gt;See the following resources to learn more about Camel Quarkus and Camel K:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Explore in more detail Camel Quarkus by reading &lt;a href="https://developers.redhat.com/articles/2021/12/06/boost-apache-camel-performance-quarkus"&gt;Boost Apache Camel performance with Quarkus&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Read the article &lt;a href="https://developers.redhat.com/articles/2021/09/14/improve-cross-team-collaboration-camel-k"&gt;Improve cross-team collaboration with Camel K&lt;/a&gt; to learn about Kamelet bindings and the DSL.&lt;/li&gt; &lt;li&gt;Learn how to implement a complete API integration using &lt;a href="https://developers.redhat.com/articles/2021/11/24/normalize-web-services-camel-k-and-atlasmap-part-1"&gt;Camel K and AtlasMap&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;A good place to start learning about Camel K is the &lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K topic page&lt;/a&gt; on Red Hat Developer.&lt;/li&gt; &lt;li&gt;Visit the &lt;a href="https://developers.redhat.com/products/integration/overview"&gt;Red Hat Integration&lt;/a&gt; page on developers.redhat.com to see complementary capabilities around Camel.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/16/choose-best-camel-your-integration-ride-part-3" title="Choose the best camel for your integration ride, Part 3"&gt;Choose the best camel for your integration ride, Part 3&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bruno Meseguer</dc:creator><dc:date>2022-03-16T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.7.5.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-7-5-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-7-5-final-released/</id><updated>2022-03-16T00:00:00Z</updated><published>2022-03-16T00:00:00Z</published><summary type="html">Today, we released 2.7.5.Final which, as usual, only contains bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.7. If you are not using 2.7 already, please refer to the 2.7 migration guide. Full changelog You can get the full changelog of 2.7.5.Final on GitHub. Come...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-03-16T00:00:00Z</dc:date></entry><entry><title type="html">Quarkus Reactive messaging with Kafka</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/quarkus-reactive-messaging-with-kafka/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/quarkus-reactive-messaging-with-kafka/</id><updated>2022-03-14T09:36:32Z</updated><content type="html">In this article, we will learn how to create a Quarkus Reactive application which uses the SmallRye Reactive Messaging and Mutiny project to stream data from and to a Kafka cluster. Reactive messaging in a nutshell The architecture style of enterprise application has been changing in the last years. Besides the standard client-server approach we ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
